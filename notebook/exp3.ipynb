{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fahim\\miniconda3\\envs\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AI Research Assistant Agent üîç\n",
      "--------------------------------\n",
      "I can help you research topics and analyze research papers.\n",
      "\n",
      "What would you like to do?\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Exit\n",
      "\n",
      "Researching 'Large Language Model'... This may take a moment.\n",
      "- Generating detailed report...\n",
      "- Finding related topics...\n",
      "\n",
      "==================================================\n",
      "RESEARCH REPORT: LARGE LANGUAGE MODEL\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Large Language Models: A Comprehensive Overview\n",
       "\n",
       "## 1. Introduction\n",
       "\n",
       "Large Language Models (LLMs) represent a significant leap forward in the field of Artificial Intelligence, specifically within Natural Language Processing (NLP). These models, typically based on deep learning architectures, are trained on vast amounts of text data, enabling them to understand, generate, and manipulate human language with impressive fluency. Their ability to perform diverse tasks, from translation and summarization to creative content generation and question answering, has made them a focal point of research and development across various industries. This report will provide a comprehensive overview of LLMs, covering their key concepts, historical context, current state, applications, and potential future directions.\n",
       "\n",
       "## 2. Key Concepts and Definitions\n",
       "\n",
       "Understanding LLMs requires familiarity with several core concepts:\n",
       "\n",
       "*   **Language Model (LM):** At its core, a language model is a probabilistic model that assigns a probability to a sequence of words. It aims to predict the likelihood of the next word in a sequence given the preceding words.  Mathematically, an LM estimates P(w1, w2, ..., wn), the probability of a sequence of words.\n",
       "\n",
       "*   **Neural Networks:** LLMs are primarily built upon neural networks, specifically deep learning architectures. These networks consist of interconnected layers of nodes (neurons) that learn complex patterns from data.\n",
       "\n",
       "*   **Deep Learning:** A subfield of machine learning that utilizes neural networks with multiple layers (deep neural networks) to analyze data with hierarchical representations. The \"depth\" of the network allows it to learn more abstract and intricate features.\n",
       "\n",
       "*   **Transformer Architecture:** The dominant architecture for modern LLMs. Introduced in the \"Attention is All You Need\" paper (Vaswani et al., 2017), transformers rely heavily on the \"attention mechanism\" to weigh the importance of different parts of the input sequence when processing information.  They excel at capturing long-range dependencies in text, overcoming limitations of earlier recurrent neural network (RNN) based models.\n",
       "\n",
       "*   **Attention Mechanism:** A core component of the transformer architecture. It allows the model to focus on the most relevant parts of the input sequence when processing each word.  Different types of attention exist, including self-attention (where the model attends to different parts of the same input sequence) and cross-attention (where the model attends to a different sequence, such as in machine translation).\n",
       "\n",
       "*   **Self-Supervised Learning:**  A training paradigm where the model learns from unlabeled data by creating its own supervision signals. For instance, an LLM might be trained to predict masked words in a sentence (masked language modeling) or to predict the next sentence in a document. This allows LLMs to leverage the vast amounts of text data available on the internet without requiring manual annotation.\n",
       "\n",
       "*   **Pre-training and Fine-tuning:** A common training strategy for LLMs.  The model is first pre-trained on a massive dataset of text to learn general language representations.  Then, it is fine-tuned on a smaller, task-specific dataset to optimize its performance for a particular application (e.g., question answering, text summarization).\n",
       "\n",
       "*   **Tokenization:** The process of breaking down text into individual units (tokens), such as words or sub-word units (e.g., using Byte-Pair Encoding (BPE)).  Tokenization is necessary for LLMs to process text numerically.\n",
       "\n",
       "*   **Embedding:** A numerical representation of words or tokens in a high-dimensional vector space. Embeddings capture semantic relationships between words, allowing the model to understand their meaning and context.\n",
       "\n",
       "*   **Context Window:** The maximum length of the input sequence that the LLM can process at once.  A larger context window allows the model to consider more information when making predictions.\n",
       "\n",
       "*   **Parameters:** The adjustable weights and biases within the neural network. The number of parameters is often used as a measure of the model's size and complexity.  LLMs are characterized by their large number of parameters, often in the billions or even trillions.\n",
       "\n",
       "*   **Emergent Abilities:** Unexpected and often surprising abilities that emerge in LLMs as they scale in size (number of parameters and training data). These abilities may not be explicitly programmed but arise from the complex interactions within the network. Examples include in-context learning, where the model can perform new tasks based on a few examples provided in the input prompt.\n",
       "\n",
       "## 3. Historical Context and Development\n",
       "\n",
       "The development of LLMs has been a gradual process, building upon decades of research in NLP and machine learning:\n",
       "\n",
       "*   **Early Language Models (pre-2010):** Early language models were primarily based on statistical methods, such as n-gram models. These models counted the frequency of word sequences and used these counts to estimate probabilities. They were limited by their inability to capture long-range dependencies and their reliance on manually engineered features.\n",
       "\n",
       "*   **Recurrent Neural Networks (RNNs) (2010s):** RNNs, particularly LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units), offered improvements over n-gram models by allowing the model to retain information about previous words in the sequence. However, RNNs struggled with long sequences due to the vanishing gradient problem.\n",
       "\n",
       "*   **Sequence-to-Sequence Models (2014):** Models like Seq2Seq with attention mechanisms, used for machine translation, were a step forward, but still suffered from limitations with long-range dependencies and parallelization.\n",
       "\n",
       "*   **The Transformer Revolution (2017-present):** The introduction of the transformer architecture in 2017 marked a turning point.  The attention mechanism allowed the model to effectively capture long-range dependencies and enabled parallel processing, leading to significant improvements in performance.\n",
       "\n",
       "    *   **BERT (Bidirectional Encoder Representations from Transformers) (2018):** BERT, developed by Google, was a groundbreaking model that used a transformer encoder to learn contextualized word embeddings. It was pre-trained on a massive corpus of text using masked language modeling and next sentence prediction.\n",
       "\n",
       "    *   **GPT (Generative Pre-trained Transformer) (2018):** GPT, developed by OpenAI, was another important model that used a transformer decoder to generate text. It was pre-trained on a large corpus of text using a causal language modeling objective (predicting the next word in a sequence).  GPT models have been iteratively improved with larger sizes and more sophisticated training techniques (GPT-2, GPT-3, GPT-4).\n",
       "\n",
       "    *   **T5 (Text-to-Text Transfer Transformer) (2019):** T5, also developed by Google, framed all NLP tasks as text-to-text problems, allowing for a unified approach to training and fine-tuning.\n",
       "\n",
       "    *   **Scaling Laws (2020):** Research on scaling laws revealed that the performance of LLMs generally improves predictably as the model size, dataset size, and compute used for training increase. This encouraged the development of even larger models.\n",
       "\n",
       "    *   **PaLM (Pathways Language Model) (2022):** Google's PaLM demonstrated advanced reasoning and few-shot learning capabilities.\n",
       "\n",
       "    *   **LLaMA (Large Language Model Meta AI) (2023):** Meta's LLaMA demonstrated strong performance with a relatively smaller size, emphasizing the importance of training data quality and efficient architectures.\n",
       "\n",
       "    *   **GPT-4 (2023):** OpenAI's GPT-4 is a multimodal model capable of processing both text and images, further expanding the capabilities of LLMs.\n",
       "\n",
       "## 4. Current State and Applications\n",
       "\n",
       "LLMs are currently deployed in a wide range of applications, transforming various industries:\n",
       "\n",
       "*   **Natural Language Understanding (NLU):**\n",
       "    *   **Sentiment Analysis:** Determining the emotional tone of text.\n",
       "    *   **Text Classification:** Categorizing text into predefined categories.\n",
       "    *   **Named Entity Recognition (NER):** Identifying and classifying named entities in text (e.g., people, organizations, locations).\n",
       "    *   **Question Answering:** Answering questions posed in natural language.\n",
       "    *   **Reading Comprehension:** Understanding and extracting information from text passages.\n",
       "\n",
       "*   **Natural Language Generation (NLG):**\n",
       "    *   **Text Summarization:** Generating concise summaries of longer texts.\n",
       "    *   **Machine Translation:** Translating text from one language to another.\n",
       "    *   **Content Creation:** Generating articles, blog posts, scripts, and other creative content.\n",
       "    *   **Chatbots and Virtual Assistants:** Creating conversational agents that can interact with users in natural language.\n",
       "    *   **Code Generation:** Generating code from natural language descriptions.\n",
       "\n",
       "*   **Search and Information Retrieval:**\n",
       "    *   **Improving Search Relevance:** Understanding the intent behind search queries and providing more relevant results.\n",
       "    *   **Knowledge Graph Construction:** Automatically extracting information from text to build knowledge graphs.\n",
       "\n",
       "*   **Healthcare:**\n",
       "    *   **Medical Diagnosis:** Assisting doctors in diagnosing diseases by analyzing patient records and medical literature.\n",
       "    *   **Drug Discovery:** Identifying potential drug candidates by analyzing scientific publications and databases.\n",
       "    *   **Personalized Medicine:** Tailoring treatment plans to individual patients based on their genetic information and medical history.\n",
       "\n",
       "*   **Finance:**\n",
       "    *   **Fraud Detection:** Identifying fraudulent transactions by analyzing financial data and news articles.\n",
       "    *   **Risk Management:** Assessing and managing financial risks by analyzing market trends and economic indicators.\n",
       "    *   **Algorithmic Trading:** Developing automated trading strategies based on market analysis.\n",
       "\n",
       "*   **Education:**\n",
       "    *   **Personalized Learning:** Tailoring educational content to individual students' needs and learning styles.\n",
       "    *   **Automated Grading:** Automating the grading of essays and other written assignments.\n",
       "    *   **Tutoring Systems:** Providing students with personalized tutoring and feedback.\n",
       "\n",
       "*   **Legal:**\n",
       "     *   **Legal Document Review:** Analyzing large volumes of legal documents to identify relevant information.\n",
       "     *   **Contract Drafting:** Assisting lawyers in drafting contracts and other legal documents.\n",
       "     *   **Legal Research:** Conducting legal research by searching and analyzing legal databases.\n",
       "\n",
       "## 5. Future Directions and Potential Developments\n",
       "\n",
       "The field of LLMs is rapidly evolving, with several promising directions for future research and development:\n",
       "\n",
       "*   **Improved Reasoning and Common Sense:** Current LLMs often struggle with tasks that require reasoning and common sense. Future research will focus on improving these capabilities by incorporating knowledge graphs, symbolic reasoning, and other techniques.\n",
       "\n",
       "*   **Multimodal Learning:** Integrating information from multiple modalities, such as text, images, audio, and video, to create more powerful and versatile models.  GPT-4 is a significant step in this direction.\n",
       "\n",
       "*   **Explainability and Interpretability:** Making LLMs more transparent and understandable, so that users can understand how they arrive at their predictions. This is crucial for building trust and ensuring responsible use.\n",
       "\n",
       "*   **Reducing Bias and Ensuring Fairness:** Addressing biases in training data and model architectures to ensure that LLMs are fair and equitable.\n",
       "\n",
       "*   **Efficient Training and Deployment:** Developing more efficient training algorithms and model architectures to reduce the computational cost and energy consumption of LLMs. This is essential for making LLMs accessible to a wider range of users and organizations. Quantization and pruning techniques are being explored.\n",
       "\n",
       "*   **Longer Context Windows:** Expanding the context window of LLMs to allow them to process longer sequences of text. This will enable them to handle more complex tasks, such as summarizing entire books or analyzing long conversations.  Architectural innovations are needed to maintain efficiency as context window size increases.\n",
       "\n",
       "*   **Continual Learning:** Enabling LLMs to continuously learn and adapt to new information without forgetting what they have already learned.\n",
       "\n",
       "*   **Personalized LLMs:** Creating LLMs that are tailored to individual users' needs and preferences. This could involve fine-tuning the model on a user's personal data or allowing the user to customize the model's behavior.\n",
       "\n",
       "*   **Agent-Based LLMs:** Using LLMs as the core component of autonomous agents that can interact with the world and perform complex tasks.  This involves equipping LLMs with tools and APIs that allow them to access and manipulate external resources.\n",
       "\n",
       "*   **Edge Computing with LLMs:** Deploying smaller, more efficient LLMs on edge devices (e.g., smartphones, wearables) to enable real-time processing of natural language data without relying on cloud connectivity.\n",
       "\n",
       "## 6. Conclusion\n",
       "\n",
       "Large Language Models represent a transformative technology with the potential to revolutionize how we interact with computers and information.  Their impressive capabilities in understanding, generating, and manipulating human language have already led to a wide range of applications across various industries.  While challenges remain in areas such as reasoning, bias, and explainability, ongoing research and development are rapidly pushing the boundaries of what is possible. As LLMs continue to evolve, they are poised to play an increasingly important role in shaping the future of AI and our relationship with technology. Their ethical implications, particularly concerning misinformation and job displacement, also require careful consideration and proactive mitigation strategies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RECOMMENDED RELATED TOPICS\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, here are 5 related topics to Large Language Models (LLMs) that a user might find interesting, along with descriptions and relevant resource links:\n",
       "\n",
       "*   **1. Topic Name:** Transformer Networks\n",
       "\n",
       "    *   **Description:** LLMs are primarily built upon the Transformer architecture. Understanding the mechanics of Transformers, including self-attention, encoder-decoder structures, and positional encoding, is crucial for comprehending how LLMs function and are trained. It allows you to dive deeper into the core of what makes LLMs tick.\n",
       "    *   **Relevant Resource Link:** [Attention is All You Need](https://arxiv.org/abs/1706.03762) - The original paper introducing the Transformer architecture.\n",
       "\n",
       "*   **2. Topic Name:** Fine-tuning LLMs\n",
       "\n",
       "    *   **Description:** Fine-tuning involves taking a pre-trained LLM and training it further on a smaller, task-specific dataset. This is a key technique for adapting general-purpose LLMs to specific applications, like sentiment analysis, question answering in a particular domain, or code generation. Understanding fine-tuning methods is essential for practically deploying LLMs.\n",
       "    *   **Relevant Resource Link:** [Hugging Face Transformers Documentation on Fine-tuning](https://huggingface.co/docs/transformers/training) - Hugging Face provides comprehensive documentation and tutorials on fine-tuning various LLMs.\n",
       "\n",
       "*   **3. Topic Name:** Prompt Engineering\n",
       "\n",
       "    *   **Description:** Prompt engineering is the art and science of crafting effective prompts (input text) to elicit desired responses from LLMs. It involves understanding how different prompts can influence the model's behavior and optimizing prompts to achieve specific goals, such as generating creative content, summarizing text, or translating languages.\n",
       "    *   **Relevant Resource Link:** [Prompt Engineering Guide](https://www.promptingguide.ai/) - A comprehensive guide covering various prompting techniques and best practices.\n",
       "\n",
       "*   **4. Topic Name:** Ethical Considerations of LLMs\n",
       "\n",
       "    *   **Description:** LLMs raise numerous ethical concerns, including bias amplification, misinformation generation, privacy violations, and job displacement. Exploring these ethical considerations is crucial for responsible development and deployment of LLMs. This includes understanding fairness, accountability, transparency, and societal impact.\n",
       "    *   **Relevant Resource Link:** [AI Ethics Resources from the Partnership on AI](https://www.partnershiponai.org/ethical-frameworks-resources/) - Offers reports and resources on the ethical implications of AI, including LLMs.\n",
       "\n",
       "*   **5. Topic Name:** Evaluation Metrics for LLMs\n",
       "\n",
       "    *   **Description:** Evaluating the performance of LLMs is a complex task. Understanding the various metrics used to assess LLMs, such as perplexity, BLEU score, ROUGE score, and human evaluation metrics, is essential for comparing different models and understanding their strengths and weaknesses. It also includes understanding the limitations of these metrics.\n",
       "    *   **Relevant Resource Link:** [Google's Research on Evaluating Large Language Models](https://ai.googleblog.com/2022/07/measuring-emergent-abilities-of-large.html) - A blog post discussing the challenges and approaches to evaluating LLMs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Full report saved to: research_Large Language Model_20250321_110246.md\n",
      "==================================================\n",
      "\n",
      "What would you like to do?\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Exit\n",
      "Thank you for using the AI Research Assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import PyPDF2 \n",
    "import docx  \n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_environment():\n",
    "    \"\"\"Load environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "    return os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "def initialize_model(api_key):\n",
    "    \"\"\"Initialize Gemini model\"\"\"\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "\n",
    "def define_output_schemas():\n",
    "    \"\"\"Define output schemas for topic and paper recommendations\"\"\"\n",
    "    # Output schema for topic recommendations\n",
    "    class RecommendedTopic(BaseModel):\n",
    "        topic: str = Field(description=\"The name of the recommended topic\")\n",
    "        description: str = Field(description=\"A brief description of why this topic is relevant\")\n",
    "        resource_url: str = Field(description=\"A relevant resource URL for this topic\")\n",
    "\n",
    "    class TopicRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedTopic] = Field(description=\"List of recommended related topics\")\n",
    "\n",
    "    # Output schema for paper recommendations\n",
    "    class RecommendedPaper(BaseModel):\n",
    "        title: str = Field(description=\"The title of the recommended research paper\")\n",
    "        authors: str = Field(description=\"The authors of the paper\")\n",
    "        year: str = Field(description=\"Publication year\")\n",
    "        description: str = Field(description=\"Brief description of relevance to the original paper\")\n",
    "        paper_url: str = Field(description=\"URL to access this paper\", default=\"\")\n",
    "\n",
    "    class PaperRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedPaper] = Field(description=\"List of recommended related papers\")\n",
    "        \n",
    "    return TopicRecommendations, PaperRecommendations\n",
    "\n",
    "def create_prompt_templates():\n",
    "    \"\"\"Create prompt templates for research tasks\"\"\"\n",
    "    report_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a comprehensive, detailed report on the following topic:\n",
    "        \n",
    "        Topic: {topic}\n",
    "        \n",
    "        Your report should include:\n",
    "        1. Introduction to the topic\n",
    "        2. Key concepts and definitions\n",
    "        3. Historical context and development\n",
    "        4. Current state and applications\n",
    "        5. Future directions and potential developments\n",
    "        6. Conclusion\n",
    "        \n",
    "        Format your report with clear markdown headings and subheadings. Use proper markdown formatting for emphasis, lists, and other elements.\n",
    "        Make sure to provide in-depth analysis.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Based on the topic: {topic}\n",
    "        \n",
    "        Generate 5 relevant related topics that the user might be interested in researching next.\n",
    "        For each recommendation, provide:\n",
    "        1. The topic name\n",
    "        2. A brief 1-2 sentence description of why it's relevant\n",
    "        3. A relevant resource URL that would contain valuable information about this topic\n",
    "        \n",
    "        Your response must be formatted as a valid JSON object that matches this structure:\n",
    "        {\n",
    "            \"recommendations\": [\n",
    "                {\n",
    "                    \"topic\": \"Topic Name\",\n",
    "                    \"description\": \"Brief description of relevance\",\n",
    "                    \"resource_url\": \"https://example.com/relevant-page\"\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        Use reputable sources for your resource URLs. While you can't verify if the exact URLs exist,\n",
    "        make them realistic and likely to contain quality information.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a concise but comprehensive summary of the following research paper:\n",
    "        \n",
    "        Paper content: {paper_content}\n",
    "        \n",
    "        Your summary should include:\n",
    "        1. Main objective of the research\n",
    "        2. Methodology used\n",
    "        3. Key findings and results\n",
    "        4. Main conclusions and implications\n",
    "        5. Limitations (if mentioned)\n",
    "        \n",
    "        Format your summary with clear markdown headings and keep it concise yet informative.\n",
    "        Focus on the most important aspects of the paper.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Based on the following research paper:\n",
    "    \n",
    "    Paper content: {paper_content}\n",
    "    \n",
    "    Generate 5 relevant related research papers that the user might be interested in reading next.\n",
    "    These should be real papers that likely exist in the academic literature.\n",
    "    \n",
    "    For each recommendation, provide:\n",
    "    1. The paper title (use the actual title of a real paper if you know it)\n",
    "    2. The authors (use \"et al.\" for multiple authors after the first)\n",
    "    3. Publication year (estimate if necessary)\n",
    "    4. A brief description of why it's relevant to the original paper\n",
    "    5. A URL where the paper might be found - THIS IS CRITICAL. \n",
    "    \n",
    "    For URLs, use specific links from:\n",
    "    - Google Scholar (https://scholar.google.com/scholar?q=PAPER_TITLE)\n",
    "    - arXiv (https://arxiv.org/search/?query=PAPER_TITLE)\n",
    "    - ResearchGate (https://www.researchgate.net/search.Search.html?query=PAPER_TITLE)\n",
    "    - ACM Digital Library (https://dl.acm.org/action/doSearch?AllField=PAPER_TITLE)\n",
    "    - IEEE Xplore (https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=PAPER_TITLE)\n",
    "    \n",
    "    Replace PAPER_TITLE with URL-encoded paper title in these templates. Make sure EVERY recommendation has a working URL.\n",
    "    \n",
    "    Your response must be formatted as a valid JSON object that matches this structure:\n",
    "    {{\n",
    "        \"recommendations\": [\n",
    "            {{\n",
    "                \"title\": \"Paper Title\",\n",
    "                \"authors\": \"Author names\",\n",
    "                \"year\": \"Publication year\",\n",
    "                \"description\": \"Brief description of relevance\",\n",
    "                \"paper_url\": \"https://example.com/paper-link\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "    \n",
    "    return report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt\n",
    "\n",
    "def create_chains(model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt, TopicRecommendations, PaperRecommendations):\n",
    "    \"\"\"Create processing chains for research tasks\"\"\"\n",
    "    report_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | report_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    recommendation_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=TopicRecommendations)\n",
    "    )\n",
    "\n",
    "    paper_summary_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_summary_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    paper_recommendation_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=PaperRecommendations)\n",
    "    )\n",
    "    \n",
    "    return report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain\n",
    "\n",
    "def initialize_database(db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Initialize SQLite database for storing papers\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            content TEXT NOT NULL,\n",
    "            file_type TEXT NOT NULL,\n",
    "            upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            summary TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_file_to_database(filename: str, content: str, file_type: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save file content to SQLite database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO papers (filename, content, file_type) VALUES (?, ?, ?)\",\n",
    "        (filename, content, file_type)\n",
    "    )\n",
    "    paper_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return paper_id\n",
    "\n",
    "def save_summary_to_database(paper_id: int, summary: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save paper summary to database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"UPDATE papers SET summary = ? WHERE id = ?\",\n",
    "        (summary, paper_id)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_paper_from_database(paper_id: int, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Retrieve paper content from database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT filename, content, file_type, summary FROM papers WHERE id = ?\", (paper_id,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    if result:\n",
    "        return {\n",
    "            \"filename\": result[0],\n",
    "            \"content\": result[1],\n",
    "            \"file_type\": result[2],\n",
    "            \"summary\": result[3]\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        # Fallback method\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a DOCX file\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from DOCX: {str(e)}\")\n",
    "        # Fallback method\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def generate_report(topic: str, report_chain) -> str:\n",
    "    \"\"\"Generate a detailed report on the given topic\"\"\"\n",
    "    return report_chain.invoke(topic)\n",
    "\n",
    "def generate_recommendations(topic: str, recommendation_chain, model) -> str:\n",
    "    \"\"\"Generate relevant topic recommendations using Gemini API\"\"\"\n",
    "    try:\n",
    "        recommendations_data = recommendation_chain.invoke(topic)\n",
    "        formatted_recommendations = \"# Related Topics You May Be Interested In\\n\\n\"\n",
    "        for i, rec in enumerate(recommendations_data.recommendations, 1):\n",
    "            formatted_recommendations += f\"## {i}. {rec.topic}\\n\"\n",
    "            formatted_recommendations += f\"{rec.description}\\n\"\n",
    "            formatted_recommendations += f\"[Learn more]({rec.resource_url})\\n\\n\"\n",
    "        return formatted_recommendations\n",
    "    except Exception as e:\n",
    "        backup_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Based on the topic: {topic}\n",
    "            \n",
    "            Provide 5 relevant related topics that the user might be interested in researching next.\n",
    "            For each recommendation, provide:\n",
    "            1. The topic name\n",
    "            2. A brief description of why it's relevant\n",
    "            3. A relevant resource link\n",
    "            \n",
    "            Format your response as a markdown list.\n",
    "            \"\"\"\n",
    "        )\n",
    "        backup_chain = backup_prompt | model | StrOutputParser()\n",
    "        return backup_chain.invoke({\"topic\": topic})\n",
    "\n",
    "def create_full_report(topic: str, report_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report combining the report and recommendations\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Report: {topic}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "{report_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This report was generated by AI Research Assistant using Gemini API*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def create_full_paper_analysis(filename: str, summary_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report for paper analysis\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Paper Analysis: {filename}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "## Paper Summary\n",
    "\n",
    "{summary_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis was generated by AI Research Assistant using Gemini API*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Convert a string to a valid filename\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "def save_markdown_file(topic: str, content: str) -> str:\n",
    "    \"\"\"Save content to a markdown file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_topic = sanitize_filename(topic)\n",
    "    filename = f\"research_{safe_topic}_{timestamp}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return filename\n",
    "\n",
    "def display_markdown(content: str, use_markdown_display: bool = True):\n",
    "    \"\"\"Display content as rendered markdown if in IPython environment\"\"\"\n",
    "    try:\n",
    "        if use_markdown_display:\n",
    "            display(Markdown(content))\n",
    "        else:\n",
    "            print(content)\n",
    "    except:\n",
    "        print(content)\n",
    "\n",
    "def process_research_paper(file_path: str, original_filename: Optional[str] = None, \n",
    "                           db_path: str = \"../data/research_papers.db\", \n",
    "                           paper_summary_chain=None, \n",
    "                           paper_recommendation_chain=None, \n",
    "                           model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Process a research paper file (PDF or DOCX)\"\"\"\n",
    "    if not original_filename:\n",
    "        original_filename = os.path.basename(file_path)\n",
    "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        text_content = extract_text_from_pdf(file_path)\n",
    "        file_type = 'pdf'\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        text_content = extract_text_from_docx(file_path)\n",
    "        file_type = 'docx'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=12000,\n",
    "        chunk_overlap=2000\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "    processing_text = chunks[0] if len(chunks) > 0 else text_content\n",
    "    paper_id = save_file_to_database(original_filename, text_content, file_type, db_path=db_path)\n",
    "    try:\n",
    "        print(\"- Generating research paper summary...\")\n",
    "        summary = paper_summary_chain.invoke(processing_text)\n",
    "        save_summary_to_database(paper_id, summary, db_path=db_path)\n",
    "        print(\"- Finding related research papers with access links...\")\n",
    "        try:\n",
    "            recommendations_data = paper_recommendation_chain.invoke(processing_text)\n",
    "            recs = recommendations_data[\"recommendations\"]  # Access as a dict\n",
    "            formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "            for i, rec in enumerate(recs, 1):\n",
    "                formatted_recommendations += f\"## {i}. {rec['title']} ({rec['year']})\\n\"\n",
    "                formatted_recommendations += f\"**Authors:** {rec['authors']}\\n\\n\"\n",
    "                formatted_recommendations += f\"{rec['description']}\\n\"\n",
    "                paper_url = rec['paper_url'].strip()\n",
    "                if not paper_url:\n",
    "                    encoded_title = re.sub(r'\\s+', '+', rec['title'])\n",
    "                    paper_url = f\"https://scholar.google.com/scholar?q={encoded_title}\"\n",
    "                formatted_recommendations += f\"[Access Paper]({paper_url})\\n\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating paper recommendations: {str(e)}\")\n",
    "            backup_prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                Based on the following research paper content:\n",
    "                \n",
    "                {paper_content}\n",
    "                \n",
    "                Provide 5 relevant related research papers that might be of interest.\n",
    "                For each paper, include:\n",
    "                1. Title (a real paper title if possible)\n",
    "                2. Authors\n",
    "                3. Year\n",
    "                4. Brief description of relevance\n",
    "                5. MOST IMPORTANTLY: A direct URL to access the paper (use Google Scholar, arXiv, or ResearchGate)\n",
    "                \n",
    "                Format your response in markdown with clear headings and clickable links.\n",
    "                Make sure every recommendation has a working URL.\n",
    "                \"\"\"\n",
    "            )\n",
    "            backup_chain = backup_prompt | model | StrOutputParser()\n",
    "            formatted_recommendations = backup_chain.invoke({\"paper_content\": processing_text})\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"summary\": summary,\n",
    "            \"recommendations\": formatted_recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing research paper: {str(e)}\")\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def run_research(topic: str, use_markdown_display: bool = True, db_path: str = \"../data/research_papers.db\",\n",
    "                report_chain=None, recommendation_chain=None, model=None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Perform research on a specific topic\"\"\"\n",
    "    if not topic.strip():\n",
    "        print(\"Please enter a valid topic.\")\n",
    "        return\n",
    "    print(f\"\\nResearching '{topic}'... This may take a moment.\")\n",
    "    try:\n",
    "        initialize_database(db_path)\n",
    "        print(\"- Generating detailed report...\")\n",
    "        report = generate_report(topic, report_chain)\n",
    "        print(\"- Finding related topics...\")\n",
    "        recommendations = generate_recommendations(topic, recommendation_chain, model)\n",
    "        full_report = create_full_report(topic, report, recommendations)\n",
    "        filename = save_markdown_file(topic, full_report)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"RESEARCH REPORT: {topic.upper()}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        display_markdown(report, use_markdown_display)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RECOMMENDED RELATED TOPICS\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        display_markdown(recommendations, use_markdown_display)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Full report saved to: {filename}\")\n",
    "        print(\"=\"*50)\n",
    "        return {\n",
    "            \"report\": report,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"full_report\": full_report,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_paper_analysis(file_path: str, original_filename: Optional[str] = None, \n",
    "                      use_markdown_display: bool = True, db_path: str = \"../data/research_papers.db\",\n",
    "                      paper_summary_chain=None, paper_recommendation_chain=None, model=None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Analyze a research paper file\"\"\"\n",
    "    try:\n",
    "        initialize_database(db_path)\n",
    "        result = process_research_paper(file_path, original_filename, db_path, \n",
    "                                       paper_summary_chain, paper_recommendation_chain, model)\n",
    "        if result[\"success\"]:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"PAPER ANALYSIS: {result['filename']}\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            display_markdown(result[\"summary\"], use_markdown_display)\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"RECOMMENDED RELATED PAPERS\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            display_markdown(result[\"recommendations\"], use_markdown_display)\n",
    "            full_analysis = create_full_paper_analysis(\n",
    "                result[\"filename\"],\n",
    "                result[\"summary\"],\n",
    "                result[\"recommendations\"]\n",
    "            )\n",
    "            analysis_filename = save_markdown_file(f\"paper_analysis_{sanitize_filename(result['filename'])}\", full_analysis)\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"Full analysis saved to: {analysis_filename}\")\n",
    "            print(\"=\"*50)\n",
    "            return {\n",
    "                \"summary\": result[\"summary\"],\n",
    "                \"recommendations\": result[\"recommendations\"],\n",
    "                \"full_analysis\": full_analysis,\n",
    "                \"filename\": analysis_filename\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to process paper: {result.get('error', 'Unknown error')}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while analyzing the paper: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_web_interface(use_markdown_display: bool = True, db_path: str = \"../data/research_papers.db\",\n",
    "                     report_chain=None, recommendation_chain=None, \n",
    "                     paper_summary_chain=None, paper_recommendation_chain=None, model=None):\n",
    "    \"\"\"Run a web interface using IPython widgets\"\"\"\n",
    "    try:\n",
    "        from ipywidgets import widgets\n",
    "        from IPython.display import display, clear_output\n",
    "        output = widgets.Output()\n",
    "        topic_input = widgets.Text(description='Topic:', placeholder='Enter research topic')\n",
    "        search_button = widgets.Button(description='Research Topic')\n",
    "        file_upload = widgets.FileUpload(\n",
    "            accept='.pdf,.docx,.doc',\n",
    "            multiple=False,\n",
    "            description='Upload Paper'\n",
    "        )\n",
    "        analyze_button = widgets.Button(description='Analyze Paper')\n",
    "        def on_search_click(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                run_research(topic_input.value, use_markdown_display, db_path, \n",
    "                           report_chain, recommendation_chain, model)\n",
    "        def on_analyze_click(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                if not file_upload.value:\n",
    "                    print(\"Please upload a research paper file (PDF or DOCX).\")\n",
    "                    return\n",
    "                file_data = next(iter(file_upload.value.values()))\n",
    "                file_name = next(iter(file_upload.value.keys()))\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as temp_file:\n",
    "                    temp_file.write(file_data['content'])\n",
    "                    temp_path = temp_file.name\n",
    "                try:\n",
    "                    run_paper_analysis(temp_path, file_name, use_markdown_display, db_path,\n",
    "                                     paper_summary_chain, paper_recommendation_chain, model)\n",
    "                finally:\n",
    "                    os.unlink(temp_path)\n",
    "        search_button.on_click(on_search_click)\n",
    "        analyze_button.on_click(on_analyze_click)\n",
    "        tab1 = widgets.VBox([topic_input, search_button])\n",
    "        tab2 = widgets.VBox([file_upload, analyze_button])\n",
    "        tabs = widgets.Tab(children=[tab1, tab2])\n",
    "        tabs.set_title(0, 'Topic Research')\n",
    "        tabs.set_title(1, 'Paper Analysis')\n",
    "        display(tabs)\n",
    "        display(output)\n",
    "    except ImportError:\n",
    "        print(\"This function requires ipywidgets. Please install with: pip install ipywidgets\")\n",
    "        print(\"Running in command line mode instead.\")\n",
    "        run()\n",
    "\n",
    "def research_topic(topic: str, report_chain=None, recommendation_chain=None, model=None):\n",
    "    \"\"\"Helper function to research a topic directly from a Jupyter notebook\"\"\"\n",
    "    return run_research(topic, use_markdown_display=True, \n",
    "                      report_chain=report_chain, recommendation_chain=recommendation_chain, model=model)\n",
    "\n",
    "def analyze_paper(file_path: str, paper_summary_chain=None, paper_recommendation_chain=None, model=None):\n",
    "    \"\"\"Helper function to analyze a paper directly from a Jupyter notebook\"\"\"\n",
    "    return run_paper_analysis(file_path, use_markdown_display=True,\n",
    "                            paper_summary_chain=paper_summary_chain, \n",
    "                            paper_recommendation_chain=paper_recommendation_chain, model=model)\n",
    "\n",
    "def run():\n",
    "    \"\"\"Main agent loop with support for both topic research and paper analysis\"\"\"\n",
    "    # Initialize components\n",
    "    api_key = load_environment()\n",
    "    model = initialize_model(api_key)\n",
    "    TopicRecommendations, PaperRecommendations = define_output_schemas()\n",
    "    report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt = create_prompt_templates()\n",
    "    report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain = create_chains(\n",
    "        model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt,\n",
    "        TopicRecommendations, PaperRecommendations\n",
    "    )\n",
    "    \n",
    "    initialize_database()\n",
    "    print(\"üîç AI Research Assistant Agent üîç\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"I can help you research topics and analyze research papers.\")\n",
    "    while True:\n",
    "        print(\"\\nWhat would you like to do?\")\n",
    "        print(\"1. Research a topic\")\n",
    "        print(\"2. Analyze a research paper\")\n",
    "        print(\"3. Exit\")\n",
    "        choice = input(\"Enter your choice (1-3): \")\n",
    "        if choice == '1':\n",
    "            topic = input(\"\\nWhat topic would you like to research? \")\n",
    "            if topic.strip():\n",
    "                run_research(topic, report_chain=report_chain, \n",
    "                           recommendation_chain=recommendation_chain, model=model)\n",
    "            else:\n",
    "                print(\"Please enter a valid topic.\")\n",
    "        elif choice == '2':\n",
    "            file_path = input(\"\\nEnter the path to the research paper file (PDF or DOCX): \")\n",
    "            if os.path.exists(file_path):\n",
    "                run_paper_analysis(file_path, paper_summary_chain=paper_summary_chain, \n",
    "                                 paper_recommendation_chain=paper_recommendation_chain, model=model)\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "        elif choice == '3':\n",
    "            print(\"Thank you for using the AI Research Assistant. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AI Research Assistant Agent üîç\n",
      "--------------------------------\n",
      "I can help you research topics, analyze research papers, and now provide an analytics dashboard for deeper insights.\n",
      "\n",
      "What would you like to do?\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Launch Analytics Dashboard for a research paper\n",
      "4. Exit\n",
      "Analyzing paper for dashboard insights...\n",
      "Launching analytics dashboard. Close the browser window to return.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21a19cf1100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to do?\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Launch Analytics Dashboard for a research paper\n",
      "4. Exit\n",
      "Thank you for using the AI Research Assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict, Any\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import PyPDF2 \n",
    "import docx  \n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For analytics dashboard\n",
    "import dash\n",
    "from dash import html, dcc, Input, Output\n",
    "import plotly.express as px\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#############################\n",
    "# Existing Functionality\n",
    "#############################\n",
    "\n",
    "def load_environment():\n",
    "    \"\"\"Load environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "    return os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "def initialize_model(api_key):\n",
    "    \"\"\"Initialize Gemini model\"\"\"\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "\n",
    "def define_output_schemas():\n",
    "    \"\"\"Define output schemas for topic and paper recommendations\"\"\"\n",
    "    # Output schema for topic recommendations\n",
    "    class RecommendedTopic(BaseModel):\n",
    "        topic: str = Field(description=\"The name of the recommended topic\")\n",
    "        description: str = Field(description=\"A brief description of why this topic is relevant\")\n",
    "        resource_url: str = Field(description=\"A relevant resource URL for this topic\")\n",
    "\n",
    "    class TopicRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedTopic] = Field(description=\"List of recommended related topics\")\n",
    "\n",
    "    # Output schema for paper recommendations\n",
    "    class RecommendedPaper(BaseModel):\n",
    "        title: str = Field(description=\"The title of the recommended research paper\")\n",
    "        authors: str = Field(description=\"The authors of the paper\")\n",
    "        year: str = Field(description=\"Publication year\")\n",
    "        description: str = Field(description=\"Brief description of relevance to the original paper\")\n",
    "        paper_url: str = Field(description=\"URL to access this paper\", default=\"\")\n",
    "\n",
    "    class PaperRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedPaper] = Field(description=\"List of recommended related papers\")\n",
    "        \n",
    "    return TopicRecommendations, PaperRecommendations\n",
    "\n",
    "def create_prompt_templates():\n",
    "    \"\"\"Create prompt templates for research tasks\"\"\"\n",
    "    report_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a comprehensive, detailed report on the following topic:\n",
    "        \n",
    "        Topic: {topic}\n",
    "        \n",
    "        Your report should include:\n",
    "        1. Introduction to the topic\n",
    "        2. Key concepts and definitions\n",
    "        3. Historical context and development\n",
    "        4. Current state and applications\n",
    "        5. Future directions and potential developments\n",
    "        6. Conclusion\n",
    "        \n",
    "        Format your report with clear markdown headings and subheadings. Use proper markdown formatting for emphasis, lists, and other elements.\n",
    "        Make sure to provide in-depth analysis.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Based on the topic: {topic}\n",
    "        \n",
    "        Generate 5 relevant related topics that the user might be interested in researching next.\n",
    "        For each recommendation, provide:\n",
    "        1. The topic name\n",
    "        2. A brief 1-2 sentence description of why it's relevant\n",
    "        3. A relevant resource URL that would contain valuable information about this topic\n",
    "        \n",
    "        Your response must be formatted as a valid JSON object that matches this structure:\n",
    "        {\n",
    "            \"recommendations\": [\n",
    "                {\n",
    "                    \"topic\": \"Topic Name\",\n",
    "                    \"description\": \"Brief description of relevance\",\n",
    "                    \"resource_url\": \"https://example.com/relevant-page\"\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        Use reputable sources for your resource URLs. While you can't verify if the exact URLs exist,\n",
    "        make them realistic and likely to contain quality information.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a concise but comprehensive summary of the following research paper:\n",
    "        \n",
    "        Paper content: {paper_content}\n",
    "        \n",
    "        Your summary should include:\n",
    "        1. Main objective of the research\n",
    "        2. Methodology used\n",
    "        3. Key findings and results\n",
    "        4. Main conclusions and implications\n",
    "        5. Limitations (if mentioned)\n",
    "        \n",
    "        Format your summary with clear markdown headings and keep it concise yet informative.\n",
    "        Focus on the most important aspects of the paper.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Based on the following research paper:\n",
    "    \n",
    "    Paper content: {paper_content}\n",
    "    \n",
    "    Generate 5 relevant related research papers that the user might be interested in reading next.\n",
    "    These should be real papers that likely exist in the academic literature.\n",
    "    \n",
    "    For each recommendation, provide:\n",
    "    1. The paper title (use the actual title of a real paper if you know it)\n",
    "    2. The authors (use \"et al.\" for multiple authors after the first)\n",
    "    3. Publication year (estimate if necessary)\n",
    "    4. A brief description of why it's relevant to the original paper\n",
    "    5. A URL where the paper might be found - THIS IS CRITICAL. \n",
    "    \n",
    "    For URLs, use specific links from:\n",
    "    - Google Scholar (https://scholar.google.com/scholar?q=PAPER_TITLE)\n",
    "    - arXiv (https://arxiv.org/search/?query=PAPER_TITLE)\n",
    "    - ResearchGate (https://www.researchgate.net/search.Search.html?query=PAPER_TITLE)\n",
    "    - ACM Digital Library (https://dl.acm.org/action/doSearch?AllField=PAPER_TITLE)\n",
    "    - IEEE Xplore (https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=PAPER_TITLE)\n",
    "    \n",
    "    Replace PAPER_TITLE with URL-encoded paper title in these templates. Make sure EVERY recommendation has a working URL.\n",
    "    \n",
    "    Your response must be formatted as a valid JSON object that matches this structure:\n",
    "    {{\n",
    "        \"recommendations\": [\n",
    "            {{\n",
    "                \"title\": \"Paper Title\",\n",
    "                \"authors\": \"Author names\",\n",
    "                \"year\": \"Publication year\",\n",
    "                \"description\": \"Brief description of relevance\",\n",
    "                \"paper_url\": \"https://example.com/paper-link\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "    \n",
    "    return report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt\n",
    "\n",
    "def create_chains(model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt, TopicRecommendations, PaperRecommendations):\n",
    "    \"\"\"Create processing chains for research tasks\"\"\"\n",
    "    report_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | report_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    recommendation_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=TopicRecommendations)\n",
    "    )\n",
    "\n",
    "    paper_summary_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_summary_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    paper_recommendation_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=PaperRecommendations)\n",
    "    )\n",
    "    \n",
    "    return report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain\n",
    "\n",
    "def initialize_database(db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Initialize SQLite database for storing papers\"\"\"\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            content TEXT NOT NULL,\n",
    "            file_type TEXT NOT NULL,\n",
    "            upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            summary TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_file_to_database(filename: str, content: str, file_type: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save file content to SQLite database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO papers (filename, content, file_type) VALUES (?, ?, ?)\",\n",
    "        (filename, content, file_type)\n",
    "    )\n",
    "    paper_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return paper_id\n",
    "\n",
    "def save_summary_to_database(paper_id: int, summary: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save paper summary to database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"UPDATE papers SET summary = ? WHERE id = ?\",\n",
    "        (summary, paper_id)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_paper_from_database(paper_id: int, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Retrieve paper content from database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT filename, content, file_type, summary FROM papers WHERE id = ?\", (paper_id,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    if result:\n",
    "        return {\n",
    "            \"filename\": result[0],\n",
    "            \"content\": result[1],\n",
    "            \"file_type\": result[2],\n",
    "            \"summary\": result[3]\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a DOCX file\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from DOCX: {str(e)}\")\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def generate_report(topic: str, report_chain) -> str:\n",
    "    \"\"\"Generate a detailed report on the given topic\"\"\"\n",
    "    return report_chain.invoke(topic)\n",
    "\n",
    "def generate_recommendations(topic: str, recommendation_chain, model) -> str:\n",
    "    \"\"\"Generate relevant topic recommendations using Gemini API\"\"\"\n",
    "    try:\n",
    "        recommendations_data = recommendation_chain.invoke(topic)\n",
    "        formatted_recommendations = \"# Related Topics You May Be Interested In\\n\\n\"\n",
    "        for i, rec in enumerate(recommendations_data.recommendations, 1):\n",
    "            formatted_recommendations += f\"## {i}. {rec.topic}\\n\"\n",
    "            formatted_recommendations += f\"{rec.description}\\n\"\n",
    "            formatted_recommendations += f\"[Learn more]({rec.resource_url})\\n\\n\"\n",
    "        return formatted_recommendations\n",
    "    except Exception as e:\n",
    "        backup_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Based on the topic: {topic}\n",
    "            \n",
    "            Provide 5 relevant related topics that the user might be interested in researching next.\n",
    "            For each recommendation, provide:\n",
    "            1. The topic name\n",
    "            2. A brief description of why it's relevant\n",
    "            3. A relevant resource link\n",
    "            \n",
    "            Format your response as a markdown list.\n",
    "            \"\"\"\n",
    "        )\n",
    "        backup_chain = backup_prompt | model | StrOutputParser()\n",
    "        return backup_chain.invoke({\"topic\": topic})\n",
    "\n",
    "def create_full_report(topic: str, report_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report combining the report and recommendations\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Report: {topic}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "{report_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This report was generated by AI Research Assistant using Gemini API*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def create_full_paper_analysis(filename: str, summary_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report for paper analysis\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Paper Analysis: {filename}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "## Paper Summary\n",
    "\n",
    "{summary_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis was generated by AI Research Assistant using Gemini API*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Convert a string to a valid filename\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "def save_markdown_file(topic: str, content: str) -> str:\n",
    "    \"\"\"Save content to a markdown file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_topic = sanitize_filename(topic)\n",
    "    filename = f\"research_{safe_topic}_{timestamp}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return filename\n",
    "\n",
    "def display_markdown(content: str, use_markdown_display: bool = True):\n",
    "    \"\"\"Display content as rendered markdown if in IPython environment\"\"\"\n",
    "    try:\n",
    "        if use_markdown_display:\n",
    "            display(Markdown(content))\n",
    "        else:\n",
    "            print(content)\n",
    "    except:\n",
    "        print(content)\n",
    "\n",
    "def process_research_paper(file_path: str, original_filename: Optional[str] = None, \n",
    "                           db_path: str = \"../data/research_papers.db\", \n",
    "                           paper_summary_chain=None, \n",
    "                           paper_recommendation_chain=None, \n",
    "                           model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Process a research paper file (PDF or DOCX)\"\"\"\n",
    "    if not original_filename:\n",
    "        original_filename = os.path.basename(file_path)\n",
    "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        text_content = extract_text_from_pdf(file_path)\n",
    "        file_type = 'pdf'\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        text_content = extract_text_from_docx(file_path)\n",
    "        file_type = 'docx'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=12000,\n",
    "        chunk_overlap=2000\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "    processing_text = chunks[0] if len(chunks) > 0 else text_content\n",
    "    paper_id = save_file_to_database(original_filename, text_content, file_type, db_path=db_path)\n",
    "    try:\n",
    "        print(\"- Generating research paper summary...\")\n",
    "        summary = paper_summary_chain.invoke(processing_text)\n",
    "        save_summary_to_database(paper_id, summary, db_path=db_path)\n",
    "        print(\"- Finding related research papers with access links...\")\n",
    "        try:\n",
    "            recommendations_data = paper_recommendation_chain.invoke(processing_text)\n",
    "            recs = recommendations_data[\"recommendations\"]  # Access as a dict\n",
    "            formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "            for i, rec in enumerate(recs, 1):\n",
    "                formatted_recommendations += f\"## {i}. {rec['title']} ({rec['year']})\\n\"\n",
    "                formatted_recommendations += f\"**Authors:** {rec['authors']}\\n\\n\"\n",
    "                formatted_recommendations += f\"{rec['description']}\\n\"\n",
    "                paper_url = rec['paper_url'].strip()\n",
    "                if not paper_url:\n",
    "                    encoded_title = re.sub(r'\\s+', '+', rec['title'])\n",
    "                    paper_url = f\"https://scholar.google.com/scholar?q={encoded_title}\"\n",
    "                formatted_recommendations += f\"[Access Paper]({paper_url})\\n\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating paper recommendations: {str(e)}\")\n",
    "            backup_prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                Based on the following research paper content:\n",
    "                \n",
    "                {paper_content}\n",
    "                \n",
    "                Provide 5 relevant related research papers that might be of interest.\n",
    "                For each paper, include:\n",
    "                1. Title (a real paper title if possible)\n",
    "                2. Authors\n",
    "                3. Year\n",
    "                4. Brief description of relevance\n",
    "                5. MOST IMPORTANTLY: A direct URL to access the paper (use Google Scholar, arXiv, or ResearchGate)\n",
    "                \n",
    "                Format your response in markdown with clear headings and clickable links.\n",
    "                Make sure every recommendation has a working URL.\n",
    "                \"\"\"\n",
    "            )\n",
    "            backup_chain = backup_prompt | model | StrOutputParser()\n",
    "            formatted_recommendations = backup_chain.invoke({\"paper_content\": processing_text})\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"summary\": summary,\n",
    "            \"recommendations\": formatted_recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing research paper: {str(e)}\")\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def run_research(topic: str, use_markdown_display: bool = True, db_path: str = \"../data/research_papers.db\",\n",
    "                report_chain=None, recommendation_chain=None, model=None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Perform research on a specific topic\"\"\"\n",
    "    if not topic.strip():\n",
    "        print(\"Please enter a valid topic.\")\n",
    "        return\n",
    "    print(f\"\\nResearching '{topic}'... This may take a moment.\")\n",
    "    try:\n",
    "        initialize_database(db_path)\n",
    "        print(\"- Generating detailed report...\")\n",
    "        report = generate_report(topic, report_chain)\n",
    "        print(\"- Finding related topics...\")\n",
    "        recommendations = generate_recommendations(topic, recommendation_chain, model)\n",
    "        full_report = create_full_report(topic, report, recommendations)\n",
    "        filename = save_markdown_file(topic, full_report)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"RESEARCH REPORT: {topic.upper()}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        display_markdown(report, use_markdown_display)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RECOMMENDED RELATED TOPICS\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        display_markdown(recommendations, use_markdown_display)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Full report saved to: {filename}\")\n",
    "        print(\"=\"*50)\n",
    "        return {\n",
    "            \"report\": report,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"full_report\": full_report,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_paper_analysis(file_path: str, original_filename: Optional[str] = None, \n",
    "                      use_markdown_display: bool = True, db_path: str = \"../data/research_papers.db\",\n",
    "                      paper_summary_chain=None, paper_recommendation_chain=None, model=None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Analyze a research paper file\"\"\"\n",
    "    try:\n",
    "        initialize_database(db_path)\n",
    "        result = process_research_paper(file_path, original_filename, db_path, \n",
    "                                       paper_summary_chain, paper_recommendation_chain, model)\n",
    "        if result[\"success\"]:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"PAPER ANALYSIS: {result['filename']}\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            display_markdown(result[\"summary\"], use_markdown_display)\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"RECOMMENDED RELATED PAPERS\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            display_markdown(result[\"recommendations\"], use_markdown_display)\n",
    "            full_analysis = create_full_paper_analysis(\n",
    "                result[\"filename\"],\n",
    "                result[\"summary\"],\n",
    "                result[\"recommendations\"]\n",
    "            )\n",
    "            analysis_filename = save_markdown_file(f\"paper_analysis_{sanitize_filename(result['filename'])}\", full_analysis)\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"Full analysis saved to: {analysis_filename}\")\n",
    "            print(\"=\"*50)\n",
    "            return {\n",
    "                \"summary\": result[\"summary\"],\n",
    "                \"recommendations\": result[\"recommendations\"],\n",
    "                \"full_analysis\": full_analysis,\n",
    "                \"filename\": analysis_filename\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Failed to process paper: {result.get('error', 'Unknown error')}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while analyzing the paper: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def research_topic(topic: str, report_chain=None, recommendation_chain=None, model=None):\n",
    "    \"\"\"Helper function to research a topic directly from a Jupyter notebook\"\"\"\n",
    "    return run_research(topic, use_markdown_display=True, \n",
    "                      report_chain=report_chain, recommendation_chain=recommendation_chain, model=model)\n",
    "\n",
    "def analyze_paper(file_path: str, paper_summary_chain=None, paper_recommendation_chain=None, model=None):\n",
    "    \"\"\"Helper function to analyze a paper directly from a Jupyter notebook\"\"\"\n",
    "    return run_paper_analysis(file_path, use_markdown_display=True,\n",
    "                            paper_summary_chain=paper_summary_chain, \n",
    "                            paper_recommendation_chain=paper_recommendation_chain, model=model)\n",
    "\n",
    "#############################\n",
    "# New Analytics Dashboard Code\n",
    "#############################\n",
    "\n",
    "def analyze_paper_for_dashboard(file_path: str, original_filename: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process the uploaded paper and extract analytics insights:\n",
    "     - Metadata extraction (for PDFs)\n",
    "     - Abstract extraction (simple regex)\n",
    "     - Keyword frequency (after removing stop words)\n",
    "     - Section breakdown (using simple heuristics)\n",
    "     - Citation count (number of 'References' occurrences)\n",
    "     - Visual elements (counts of \"Figure\" and \"Table\")\n",
    "     - Readability (word count and average sentence length)\n",
    "     - Future work / Limitations (attempt to extract the 'Future Work' section)\n",
    "    \"\"\"\n",
    "    if not original_filename:\n",
    "        original_filename = os.path.basename(file_path)\n",
    "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        text_content = extract_text_from_pdf(file_path)\n",
    "        # Try to extract metadata using PyPDF2\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                metadata = reader.metadata\n",
    "                metadata_dict = {\n",
    "                    \"Title\": metadata.title if metadata.title else \"N/A\",\n",
    "                    \"Author\": metadata.author if metadata.author else \"N/A\",\n",
    "                    \"CreationDate\": metadata.creation_date if metadata.creation_date else \"N/A\",\n",
    "                    \"Producer\": metadata.producer if metadata.producer else \"N/A\",\n",
    "                    \"Subject\": metadata.subject if metadata.subject else \"N/A\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            metadata_dict = {}\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        text_content = extract_text_from_docx(file_path)\n",
    "        metadata_dict = {}  # DOCX metadata extraction can be added if needed\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "\n",
    "    # Basic abstract extraction: look for the word \"Abstract\" and take the following 300 words.\n",
    "    abstract = \"\"\n",
    "    match = re.search(r'Abstract\\s*[:\\-]?\\s*(.*)', text_content, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        abstract = \" \".join(match.group(1).split()[:300])\n",
    "    else:\n",
    "        abstract = \"Abstract not found.\"\n",
    "\n",
    "    # Keyword extraction: simple frequency analysis excluding common stopwords\n",
    "    stopwords = set([\"the\", \"and\", \"is\", \"in\", \"of\", \"to\", \"a\", \"for\", \"with\", \"that\", \"as\", \"on\", \"by\", \"an\", \"are\"])\n",
    "    words = [word.strip(string.punctuation).lower() for word in text_content.split()]\n",
    "    filtered_words = [w for w in words if w and w not in stopwords and len(w) > 3]\n",
    "    word_counts = Counter(filtered_words)\n",
    "    most_common = word_counts.most_common(10)\n",
    "    keywords = [{\"word\": word, \"count\": count} for word, count in most_common]\n",
    "\n",
    "    # Section breakdown: try to count common sections (heuristic based on headings)\n",
    "    sections = {}\n",
    "    for sec in [\"Introduction\", \"Methods\", \"Results\", \"Discussion\", \"Conclusion\"]:\n",
    "        pattern = re.compile(rf'{sec}', re.IGNORECASE)\n",
    "        count = len(pattern.findall(text_content))\n",
    "        sections[sec] = count\n",
    "\n",
    "    # Citation analysis: count the occurrence of the word \"Reference\" (as a proxy)\n",
    "    citations = text_content.lower().count(\"reference\")\n",
    "\n",
    "    # Visual elements: count \"Figure\" and \"Table\"\n",
    "    figures = text_content.lower().count(\"figure\")\n",
    "    tables = text_content.lower().count(\"table\")\n",
    "\n",
    "    # Readability & text analytics\n",
    "    words_total = len(words)\n",
    "    sentences = re.split(r'[.!?]+', text_content)\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "    avg_sentence_length = words_total / len(sentences) if sentences else 0\n",
    "\n",
    "    # Future work / limitations: simple extraction if exists\n",
    "    future_work = \"\"\n",
    "    match_future = re.search(r'(Future Work|Limitations)\\s*[:\\-]?\\s*(.*)', text_content, re.IGNORECASE | re.DOTALL)\n",
    "    if match_future:\n",
    "        future_work = \" \".join(match_future.group(2).split()[:100])\n",
    "    else:\n",
    "        future_work = \"Not clearly mentioned.\"\n",
    "\n",
    "    analytics_data = {\n",
    "        \"metadata\": metadata_dict,\n",
    "        \"abstract\": abstract,\n",
    "        \"keywords\": keywords,\n",
    "        \"sections\": sections,\n",
    "        \"citations\": citations,\n",
    "        \"figures\": figures,\n",
    "        \"tables\": tables,\n",
    "        \"word_count\": words_total,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"future_work\": future_work\n",
    "    }\n",
    "    return analytics_data\n",
    "\n",
    "def create_analytics_dashboard_app(analytics_data: Dict[str, Any]) -> dash.Dash:\n",
    "    \"\"\"Create a Dash app to visualize the analytics data.\"\"\"\n",
    "    app = dash.Dash(__name__)\n",
    "    # Metadata table as a simple HTML list\n",
    "    metadata_items = [html.Li(f\"{k}: {v}\") for k, v in analytics_data[\"metadata\"].items()] if analytics_data[\"metadata\"] else [html.Li(\"No metadata available\")]\n",
    "    # Keyword bar chart\n",
    "    kw_df = {\"Word\": [item[\"word\"] for item in analytics_data[\"keywords\"]],\n",
    "             \"Count\": [item[\"count\"] for item in analytics_data[\"keywords\"]]}\n",
    "    fig_keywords = px.bar(kw_df, x=\"Word\", y=\"Count\", title=\"Top Keywords\")\n",
    "    # Section breakdown pie chart\n",
    "    sec_df = {\"Section\": list(analytics_data[\"sections\"].keys()),\n",
    "              \"Count\": list(analytics_data[\"sections\"].values())}\n",
    "    fig_sections = px.pie(sec_df, names=\"Section\", values=\"Count\", title=\"Section Occurrences\")\n",
    "    # Visual elements summary\n",
    "    visual_summary = f\"Figures: {analytics_data['figures']} | Tables: {analytics_data['tables']}\"\n",
    "    # Layout\n",
    "    app.layout = html.Div([\n",
    "        html.H1(\"Research Paper Analytics Dashboard\"),\n",
    "        html.H2(\"Metadata Overview\"),\n",
    "        html.Ul(metadata_items),\n",
    "        html.H2(\"Abstract & Summary Analysis\"),\n",
    "        html.P(analytics_data[\"abstract\"]),\n",
    "        html.H2(\"Keyword & Topic Extraction\"),\n",
    "        dcc.Graph(figure=fig_keywords),\n",
    "        html.H2(\"Section Breakdown\"),\n",
    "        dcc.Graph(figure=fig_sections),\n",
    "        html.H2(\"Citation & Bibliometric Analysis\"),\n",
    "        html.P(f\"Total References (proxy count): {analytics_data['citations']}\"),\n",
    "        html.H2(\"Visual Elements Extraction\"),\n",
    "        html.P(visual_summary),\n",
    "        html.H2(\"Readability & Text Analytics\"),\n",
    "        html.P(f\"Word Count: {analytics_data['word_count']}\"),\n",
    "        html.P(f\"Average Sentence Length: {analytics_data['avg_sentence_length']:.2f} words\"),\n",
    "        html.H2(\"Future Work & Limitations\"),\n",
    "        html.P(analytics_data[\"future_work\"])\n",
    "    ])\n",
    "    return app\n",
    "\n",
    "def run_analytics_dashboard(file_path: str, original_filename: Optional[str] = None):\n",
    "    \"\"\"Process the uploaded paper and launch the analytics dashboard.\"\"\"\n",
    "    print(\"Analyzing paper for dashboard insights...\")\n",
    "    analytics_data = analyze_paper_for_dashboard(file_path, original_filename)\n",
    "    app = create_analytics_dashboard_app(analytics_data)\n",
    "    print(\"Launching analytics dashboard. Close the browser window to return.\")\n",
    "    # Running on a local server; adjust debug and port as needed.\n",
    "    app.run(debug=True, port=8050)\n",
    "\n",
    "\n",
    "#############################\n",
    "# Web Interface with 3 Tabs\n",
    "#############################\n",
    "\n",
    "def run_web_interface(use_markdown_display: bool = True, db_path: str = \"../data/research_papers.db\",\n",
    "                     report_chain=None, recommendation_chain=None, \n",
    "                     paper_summary_chain=None, paper_recommendation_chain=None, model=None):\n",
    "    \"\"\"Run a web interface using IPython widgets with three tabs: Topic Research, Paper Analysis, and Analytics Dashboard.\"\"\"\n",
    "    try:\n",
    "        from ipywidgets import widgets\n",
    "        from IPython.display import display, clear_output\n",
    "        output = widgets.Output()\n",
    "\n",
    "        # Tab 1: Topic Research\n",
    "        topic_input = widgets.Text(description='Topic:', placeholder='Enter research topic')\n",
    "        search_button = widgets.Button(description='Research Topic')\n",
    "        def on_search_click(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                run_research(topic_input.value, use_markdown_display, db_path, \n",
    "                           report_chain, recommendation_chain, model)\n",
    "        search_button.on_click(on_search_click)\n",
    "        tab1 = widgets.VBox([topic_input, search_button])\n",
    "        \n",
    "        # Tab 2: Paper Analysis\n",
    "        file_upload_analysis = widgets.FileUpload(\n",
    "            accept='.pdf,.docx,.doc',\n",
    "            multiple=False,\n",
    "            description='Upload Paper'\n",
    "        )\n",
    "        analyze_button = widgets.Button(description='Analyze Paper')\n",
    "        def on_analyze_click(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                if not file_upload_analysis.value:\n",
    "                    print(\"Please upload a research paper file (PDF or DOCX).\")\n",
    "                    return\n",
    "                file_data = next(iter(file_upload_analysis.value.values()))\n",
    "                file_name = next(iter(file_upload_analysis.value.keys()))\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as temp_file:\n",
    "                    temp_file.write(file_data['content'])\n",
    "                    temp_path = temp_file.name\n",
    "                try:\n",
    "                    run_paper_analysis(temp_path, file_name, use_markdown_display, db_path,\n",
    "                                     paper_summary_chain, paper_recommendation_chain, model)\n",
    "                finally:\n",
    "                    os.unlink(temp_path)\n",
    "        analyze_button.on_click(on_analyze_click)\n",
    "        tab2 = widgets.VBox([file_upload_analysis, analyze_button])\n",
    "        \n",
    "        # Tab 3: Analytics Dashboard\n",
    "        file_upload_dashboard = widgets.FileUpload(\n",
    "            accept='.pdf,.docx,.doc',\n",
    "            multiple=False,\n",
    "            description='Upload Paper'\n",
    "        )\n",
    "        dashboard_button = widgets.Button(description='Show Analytics Dashboard')\n",
    "        def on_dashboard_click(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                if not file_upload_dashboard.value:\n",
    "                    print(\"Please upload a research paper file (PDF or DOCX) for analytics.\")\n",
    "                    return\n",
    "                file_data = next(iter(file_upload_dashboard.value.values()))\n",
    "                file_name = next(iter(file_upload_dashboard.value.keys()))\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1]) as temp_file:\n",
    "                    temp_file.write(file_data['content'])\n",
    "                    temp_path = temp_file.name\n",
    "                try:\n",
    "                    # This function will start a Dash server and open the dashboard in your browser.\n",
    "                    run_analytics_dashboard(temp_path, file_name)\n",
    "                finally:\n",
    "                    os.unlink(temp_path)\n",
    "        dashboard_button.on_click(on_dashboard_click)\n",
    "        tab3 = widgets.VBox([file_upload_dashboard, dashboard_button])\n",
    "        \n",
    "        tabs = widgets.Tab(children=[tab1, tab2, tab3])\n",
    "        tabs.set_title(0, 'Topic Research')\n",
    "        tabs.set_title(1, 'Paper Analysis')\n",
    "        tabs.set_title(2, 'Analytics Dashboard')\n",
    "        display(tabs)\n",
    "        display(output)\n",
    "    except ImportError:\n",
    "        print(\"This function requires ipywidgets. Please install with: pip install ipywidgets\")\n",
    "        print(\"Running in command line mode instead.\")\n",
    "        run()\n",
    "\n",
    "def run():\n",
    "    \"\"\"Main agent loop with support for topic research, paper analysis, and analytics dashboard.\"\"\"\n",
    "    # Initialize components\n",
    "    api_key = load_environment()\n",
    "    model = initialize_model(api_key)\n",
    "    TopicRecommendations, PaperRecommendations = define_output_schemas()\n",
    "    report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt = create_prompt_templates()\n",
    "    report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain = create_chains(\n",
    "        model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt,\n",
    "        TopicRecommendations, PaperRecommendations\n",
    "    )\n",
    "    \n",
    "    initialize_database()\n",
    "    print(\"üîç AI Research Assistant Agent üîç\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"I can help you research topics, analyze research papers, and now provide an analytics dashboard for deeper insights.\")\n",
    "    while True:\n",
    "        print(\"\\nWhat would you like to do?\")\n",
    "        print(\"1. Research a topic\")\n",
    "        print(\"2. Analyze a research paper\")\n",
    "        print(\"3. Launch Analytics Dashboard for a research paper\")\n",
    "        print(\"4. Exit\")\n",
    "        choice = input(\"Enter your choice (1-4): \")\n",
    "        if choice == '1':\n",
    "            topic = input(\"\\nWhat topic would you like to research? \")\n",
    "            if topic.strip():\n",
    "                run_research(topic, report_chain=report_chain, \n",
    "                           recommendation_chain=recommendation_chain, model=model)\n",
    "            else:\n",
    "                print(\"Please enter a valid topic.\")\n",
    "        elif choice == '2':\n",
    "            file_path = input(\"\\nEnter the path to the research paper file (PDF or DOCX): \")\n",
    "            if os.path.exists(file_path):\n",
    "                run_paper_analysis(file_path, paper_summary_chain=paper_summary_chain, \n",
    "                                 paper_recommendation_chain=paper_recommendation_chain, model=model)\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "        elif choice == '3':\n",
    "            file_path = input(\"\\nEnter the path to the research paper file (PDF or DOCX) for analytics: \")\n",
    "            if os.path.exists(file_path):\n",
    "                run_analytics_dashboard(file_path)\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "        elif choice == '4':\n",
    "            print(\"Thank you for using the AI Research Assistant. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, 3, or 4.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
