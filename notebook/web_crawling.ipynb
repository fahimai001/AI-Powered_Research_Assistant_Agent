{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] {research,analyze,paper} ...\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\fahim\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3b5ad2c945842d6cf304a5d0565d67860bed15d7a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import json\n",
    "from urllib.parse import quote_plus\n",
    "import time\n",
    "import random\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import PyPDF2 \n",
    "import docx  \n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0.7,\n",
    "    max_tokens=4000,\n",
    ")\n",
    "\n",
    "# Create output schema for topic recommendations\n",
    "class RecommendedTopic(BaseModel):\n",
    "    topic: str = Field(description=\"The name of the recommended topic\")\n",
    "    description: str = Field(description=\"A brief description of why this topic is relevant\")\n",
    "    resource_url: str = Field(description=\"A relevant resource URL for this topic\")\n",
    "\n",
    "class TopicRecommendations(BaseModel):\n",
    "    recommendations: List[RecommendedTopic] = Field(description=\"List of recommended related topics\")\n",
    "\n",
    "# Create output schema for paper recommendations\n",
    "class RecommendedPaper(BaseModel):\n",
    "    title: str = Field(description=\"The title of the recommended research paper\")\n",
    "    authors: str = Field(description=\"The authors of the paper\")\n",
    "    year: str = Field(description=\"Publication year\")\n",
    "    description: str = Field(description=\"Brief description of relevance to the original paper\")\n",
    "    paper_url: str = Field(description=\"URL to access this paper\", default=\"\")\n",
    "\n",
    "class PaperRecommendations(BaseModel):\n",
    "    recommendations: List[RecommendedPaper] = Field(description=\"List of recommended related papers\")\n",
    "\n",
    "# Create prompt templates\n",
    "report_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI research assistant. Create a comprehensive, detailed report on the following topic:\n",
    "    \n",
    "    Topic: {topic}\n",
    "    \n",
    "    Your report should include:\n",
    "    1. Introduction to the topic\n",
    "    2. Key concepts and definitions\n",
    "    3. Historical context and development\n",
    "    4. Current state and applications\n",
    "    5. Future directions and potential developments\n",
    "    6. Conclusion\n",
    "    \n",
    "    Format your report with clear markdown headings and subheadings. Use proper markdown formatting for emphasis, lists, and other elements.\n",
    "    Make sure to provide in-depth analysis.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "paper_summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI research assistant. Create a concise but comprehensive summary of the following research paper:\n",
    "    \n",
    "    Paper content: {paper_content}\n",
    "    \n",
    "    Your summary should include:\n",
    "    1. Main objective of the research\n",
    "    2. Methodology used\n",
    "    3. Key findings and results\n",
    "    4. Main conclusions and implications\n",
    "    5. Limitations (if mentioned)\n",
    "    \n",
    "    Format your summary with clear markdown headings and keep it concise yet informative.\n",
    "    Focus on the most important aspects of the paper.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "report_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | report_prompt\n",
    "    | gemini_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "paper_summary_chain = (\n",
    "    {\"paper_content\": RunnablePassthrough()}\n",
    "    | paper_summary_prompt\n",
    "    | gemini_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --------------------- Web Crawling Functions ---------------------\n",
    "def get_user_agent():\n",
    "    \"\"\"Get a random user agent to avoid detection\"\"\"\n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        return ua.random\n",
    "    except:\n",
    "        # Fallback user agents if fake_useragent fails\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "        ]\n",
    "        return random.choice(user_agents)\n",
    "\n",
    "def make_request(url, max_retries=3, timeout=10):\n",
    "    \"\"\"Make a request to a URL with retries and random delays\"\"\"\n",
    "    headers = {\"User-Agent\": get_user_agent()}\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Add a small random delay to avoid rate limiting\n",
    "            time.sleep(random.uniform(0.5, 2.0))\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to retrieve {url}: {e}\")\n",
    "                return None\n",
    "            # Exponential backoff\n",
    "            time.sleep(2 ** attempt)\n",
    "    return None\n",
    "\n",
    "def extract_text_from_soup(soup):\n",
    "    \"\"\"Extract clean text from BeautifulSoup object\"\"\"\n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text\n",
    "    text = soup.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    # Clean text\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_metadata_from_soup(soup):\n",
    "    \"\"\"Extract metadata like title, description from BeautifulSoup object\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        metadata['title'] = title_tag.text.strip()\n",
    "    \n",
    "    # Extract meta description\n",
    "    description_tag = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})\n",
    "    if description_tag and description_tag.get('content'):\n",
    "        metadata['description'] = description_tag.get('content').strip()\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def crawl_website(url):\n",
    "    \"\"\"Crawl a website and return text content and metadata\"\"\"\n",
    "    response = make_request(url)\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = extract_text_from_soup(soup)\n",
    "    metadata = extract_metadata_from_soup(soup)\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'text': text,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "def search_google(query, num_results=10):\n",
    "    \"\"\"Search Google and return a list of search result URLs\"\"\"\n",
    "    # Use Google Search API or scrape Google Search results\n",
    "    # This is a placeholder implementation using direct Google search\n",
    "    # In a production environment, you might want to use a proper API\n",
    "    \n",
    "    # Encode the query for URL\n",
    "    encoded_query = quote_plus(query)\n",
    "    search_url = f\"https://www.google.com/search?q={encoded_query}\"\n",
    "    \n",
    "    response = make_request(search_url)\n",
    "    if not response:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract links from search results\n",
    "    results = []\n",
    "    for g in soup.find_all('div', class_='g'):\n",
    "        link_elements = g.find_all('a')\n",
    "        for link in link_elements:\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and not href.startswith('https://www.google.com'):\n",
    "                results.append(href)\n",
    "                if len(results) >= num_results:\n",
    "                    return results\n",
    "    \n",
    "    return results\n",
    "\n",
    "def search_academic_sources(query, num_results=10):\n",
    "    \"\"\"Search academic sources like Google Scholar and return paper URLs\"\"\"\n",
    "    # Encode the query for URL\n",
    "    encoded_query = quote_plus(query)\n",
    "    \n",
    "    # Search Google Scholar\n",
    "    scholar_url = f\"https://scholar.google.com/scholar?q={encoded_query}\"\n",
    "    response = make_request(scholar_url)\n",
    "    \n",
    "    if not response:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract paper information\n",
    "    papers = []\n",
    "    for result in soup.find_all('div', class_='gs_ri'):\n",
    "        try:\n",
    "            title_element = result.find('h3', class_='gs_rt')\n",
    "            title = title_element.text if title_element else \"Unknown Title\"\n",
    "            \n",
    "            authors_year_element = result.find('div', class_='gs_a')\n",
    "            authors_year_text = authors_year_element.text if authors_year_element else \"\"\n",
    "            \n",
    "            # Extract authors and year\n",
    "            authors = authors_year_text.split('-')[0].strip() if '-' in authors_year_text else authors_year_text\n",
    "            year_match = re.search(r'\\b(19|20)\\d{2}\\b', authors_year_text)\n",
    "            year = year_match.group(0) if year_match else \"Unknown\"\n",
    "            \n",
    "            # Get the paper URL\n",
    "            link_element = title_element.find('a') if title_element else None\n",
    "            url = link_element.get('href') if link_element else f\"https://scholar.google.com/scholar?q={quote_plus(title)}\"\n",
    "            \n",
    "            papers.append({\n",
    "                'title': title,\n",
    "                'authors': authors,\n",
    "                'year': year,\n",
    "                'url': url\n",
    "            })\n",
    "            \n",
    "            if len(papers) >= num_results:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing Google Scholar result: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # If we didn't get enough results, try other sources\n",
    "    if len(papers) < num_results:\n",
    "        # Try arXiv\n",
    "        arxiv_url = f\"https://arxiv.org/search/?query={encoded_query}&searchtype=all\"\n",
    "        response = make_request(arxiv_url)\n",
    "        \n",
    "        if response:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            for result in soup.find_all('li', class_='arxiv-result'):\n",
    "                try:\n",
    "                    title_element = result.find('p', class_='title')\n",
    "                    title = title_element.text.strip() if title_element else \"Unknown Title\"\n",
    "                    \n",
    "                    authors_element = result.find('p', class_='authors')\n",
    "                    authors = authors_element.text.replace('Authors:', '').strip() if authors_element else \"Unknown Authors\"\n",
    "                    \n",
    "                    # Extract year from the submission date\n",
    "                    submitted_element = result.find('p', class_='submitted')\n",
    "                    year = \"Unknown\"\n",
    "                    if submitted_element:\n",
    "                        year_match = re.search(r'\\b(19|20)\\d{2}\\b', submitted_element.text)\n",
    "                        year = year_match.group(0) if year_match else \"Unknown\"\n",
    "                    \n",
    "                    # Get the paper URL\n",
    "                    link_element = result.find('a', title='Abstract')\n",
    "                    url = \"https://arxiv.org\" + link_element.get('href') if link_element else f\"https://arxiv.org/search/?query={quote_plus(title)}\"\n",
    "                    \n",
    "                    papers.append({\n",
    "                        'title': title,\n",
    "                        'authors': authors,\n",
    "                        'year': year,\n",
    "                        'url': url\n",
    "                    })\n",
    "                    \n",
    "                    if len(papers) >= num_results:\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing arXiv result: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    return papers\n",
    "\n",
    "def get_topic_recommendations(topic, num_recommendations=5):\n",
    "    \"\"\"Use web crawling to find related topics and resources\"\"\"\n",
    "    # First, search for the topic\n",
    "    search_results = search_google(f\"{topic} related topics\", num_results=15)\n",
    "    \n",
    "    # Crawl the top results to find related topics\n",
    "    crawled_data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_url = {executor.submit(crawl_website, url): url for url in search_results[:10]}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data:\n",
    "                    crawled_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling website: {e}\")\n",
    "    \n",
    "    # If we didn't get enough data, try another search\n",
    "    if len(crawled_data) < 3:\n",
    "        alternative_search = search_google(f\"{topic} guide tutorial\", num_results=10)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future_to_url = {executor.submit(crawl_website, url): url for url in alternative_search[:5]}\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data:\n",
    "                        crawled_data.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error crawling website: {e}\")\n",
    "    \n",
    "    # Extract potential related topics from the crawled data\n",
    "    related_topics = []\n",
    "    for data in crawled_data:\n",
    "        url = data['url']\n",
    "        site_title = data['metadata'].get('title', '')\n",
    "        site_description = data['metadata'].get('description', '')\n",
    "        \n",
    "        # Skip if the site is not relevant\n",
    "        if not any(keyword in site_title.lower() or keyword in site_description.lower() for keyword in topic.lower().split()):\n",
    "            continue\n",
    "        \n",
    "        # Create a recommendation\n",
    "        topic_name = site_title.replace(\" - \", \" \").split(\" | \")[0].strip()\n",
    "        \n",
    "        # Avoid duplicate topics\n",
    "        if any(topic_name.lower() == existing['topic'].lower() for existing in related_topics):\n",
    "            continue\n",
    "        \n",
    "        recommendation = {\n",
    "            'topic': topic_name,\n",
    "            'description': site_description[:150] + \"...\" if len(site_description) > 150 else site_description,\n",
    "            'resource_url': url\n",
    "        }\n",
    "        \n",
    "        related_topics.append(recommendation)\n",
    "        \n",
    "        if len(related_topics) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    # If we didn't get enough recommendations, create some from the topic itself\n",
    "    while len(related_topics) < num_recommendations:\n",
    "        # Use the topic name to generate a fake related topic\n",
    "        fake_topic = f\"{topic} {['applications', 'implementations', 'examples', 'tutorials', 'case studies'][len(related_topics) % 5]}\"\n",
    "        related_topics.append({\n",
    "            'topic': fake_topic,\n",
    "            'description': f\"Learn more about {fake_topic} and how it relates to {topic}.\",\n",
    "            'resource_url': f\"https://www.google.com/search?q={quote_plus(fake_topic)}\"\n",
    "        })\n",
    "    \n",
    "    return related_topics\n",
    "\n",
    "def get_paper_recommendations(paper_content, num_recommendations=5):\n",
    "    \"\"\"Use web crawling to find related research papers\"\"\"\n",
    "    # Extract key terms from the paper content\n",
    "    key_terms = extract_key_terms(paper_content)\n",
    "    \n",
    "    # Search for related papers\n",
    "    papers = []\n",
    "    for term in key_terms[:3]:  # Use the top 3 key terms\n",
    "        term_papers = search_academic_sources(term, num_results=5)\n",
    "        papers.extend(term_papers)\n",
    "        \n",
    "        if len(papers) >= num_recommendations * 2:  # Get more than needed to filter\n",
    "            break\n",
    "    \n",
    "    # Remove duplicates based on title\n",
    "    unique_papers = []\n",
    "    seen_titles = set()\n",
    "    for paper in papers:\n",
    "        title_lower = paper['title'].lower()\n",
    "        if title_lower not in seen_titles:\n",
    "            seen_titles.add(title_lower)\n",
    "            unique_papers.append(paper)\n",
    "    \n",
    "    # Take the top N papers\n",
    "    recommended_papers = unique_papers[:num_recommendations]\n",
    "    \n",
    "    # Generate descriptions for the papers\n",
    "    for paper in recommended_papers:\n",
    "        paper['description'] = generate_paper_description(paper['title'], key_terms)\n",
    "    \n",
    "    return recommended_papers\n",
    "\n",
    "def extract_key_terms(text):\n",
    "    \"\"\"Extract key terms from text using simple frequency analysis\"\"\"\n",
    "    # Remove common words and punctuation\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    \n",
    "    # Filter out common stop words\n",
    "    stop_words = set(['the', 'and', 'is', 'in', 'to', 'of', 'for', 'a', 'with', 'as', 'an', 'by', 'on', 'are', 'that', 'this', 'it', 'from', 'be', 'was', 'were', 'we', 'they', 'our', 'their', 'these', 'those', 'has', 'have', 'had', 'not', 'can', 'will', 'should', 'would', 'could', 'may', 'might', 'must', 'shall', 'which', 'what', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how'])\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = {}\n",
    "    for word in filtered_words:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract multi-word terms\n",
    "    bigrams = []\n",
    "    for i in range(len(filtered_words) - 1):\n",
    "        bigrams.append(filtered_words[i] + \" \" + filtered_words[i+1])\n",
    "    \n",
    "    # Count bigram frequencies\n",
    "    bigram_counts = {}\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_bigrams = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Combine top single words and bigrams\n",
    "    key_terms = [word for word, _ in sorted_words[:10]]\n",
    "    key_terms.extend([bigram for bigram, _ in sorted_bigrams[:5]])\n",
    "    \n",
    "    return key_terms\n",
    "\n",
    "def generate_paper_description(title, key_terms):\n",
    "    \"\"\"Generate a description for a related paper based on the key terms\"\"\"\n",
    "    # Check if the title contains any of the key terms\n",
    "    matching_terms = [term for term in key_terms if term.lower() in title.lower()]\n",
    "    \n",
    "    if matching_terms:\n",
    "        term = matching_terms[0]\n",
    "        return f\"This paper explores {term} which is directly related to the core concepts in your research. It offers additional insights and perspectives that could complement your work.\"\n",
    "    else:\n",
    "        return f\"This paper provides complementary research that relates to your work. It discusses concepts and methodologies that could enhance your understanding of the subject matter.\"\n",
    "\n",
    "# --------------------- Database Functions ---------------------\n",
    "def initialize_database(db_path: str = \"research_papers.db\"):\n",
    "    \"\"\"Initialize SQLite database for storing papers\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            content TEXT NOT NULL,\n",
    "            file_type TEXT NOT NULL,\n",
    "            upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            summary TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_file_to_database(filename: str, content: str, file_type: str, db_path: str = \"research_papers.db\"):\n",
    "    \"\"\"Save file content to SQLite database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO papers (filename, content, file_type) VALUES (?, ?, ?)\",\n",
    "        (filename, content, file_type)\n",
    "    )\n",
    "    paper_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return paper_id\n",
    "\n",
    "def save_summary_to_database(paper_id: int, summary: str, db_path: str = \"research_papers.db\"):\n",
    "    \"\"\"Save paper summary to database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"UPDATE papers SET summary = ? WHERE id = ?\",\n",
    "        (summary, paper_id)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_paper_from_database(paper_id: int, db_path: str = \"research_papers.db\"):\n",
    "    \"\"\"Retrieve paper content from database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT filename, content, file_type, summary FROM papers WHERE id = ?\", (paper_id,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    if result:\n",
    "        return {\n",
    "            \"filename\": result[0],\n",
    "            \"content\": result[1],\n",
    "            \"file_type\": result[2],\n",
    "            \"summary\": result[3]\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# --------------------- Text Extraction Functions ---------------------\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        # Fallback method\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a DOCX file\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from DOCX: {str(e)}\")\n",
    "        # Fallback method\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "# --------------------- Report and Recommendation Functions ---------------------\n",
    "def generate_report(topic: str) -> str:\n",
    "    \"\"\"Generate a detailed report on the given topic\"\"\"\n",
    "    return report_chain.invoke(topic)\n",
    "\n",
    "def generate_recommendations(topic: str) -> str:\n",
    "    \"\"\"Generate relevant topic recommendations using web crawling\"\"\"\n",
    "    try:\n",
    "        recommendations = get_topic_recommendations(topic)\n",
    "        formatted_recommendations = \"# Related Topics You May Be Interested In\\n\\n\"\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            formatted_recommendations += f\"## {i}. {rec['topic']}\\n\"\n",
    "            formatted_recommendations += f\"{rec['description']}\\n\"\n",
    "            formatted_recommendations += f\"[Learn more]({rec['resource_url']})\\n\\n\"\n",
    "        return formatted_recommendations\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating recommendations: {str(e)}\")\n",
    "        # Fallback to basic recommendations\n",
    "        formatted_recommendations = \"# Related Topics You May Be Interested In\\n\\n\"\n",
    "        for i in range(1, 6):\n",
    "            formatted_recommendations += f\"## {i}. {topic} {'applications' if i == 1 else 'examples' if i == 2 else 'tutorials' if i == 3 else 'case studies' if i == 4 else 'best practices'}\\n\"\n",
    "            formatted_recommendations += f\"Learn more about {topic} and related concepts.\\n\"\n",
    "            formatted_recommendations += f\"[Learn more](https://www.google.com/search?q={quote_plus(topic + ' ' + ('applications' if i == 1 else 'examples' if i == 2 else 'tutorials' if i == 3 else 'case studies' if i == 4 else 'best practices'))})\\n\\n\"\n",
    "        return formatted_recommendations\n",
    "\n",
    "def create_full_report(topic: str, report_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report combining the report and recommendations\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Report: {topic}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "{report_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This report was generated by AI Research Assistant using web crawling for recommendations*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def generate_paper_recommendations_content(paper_content: str) -> str:\n",
    "    \"\"\"Generate paper recommendations content using web crawling\"\"\"\n",
    "    try:\n",
    "        recommendations = get_paper_recommendations(paper_content)\n",
    "        formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            formatted_recommendations += f\"## {i}. {rec['title']} ({rec['year']})\\n\"\n",
    "            formatted_recommendations += f\"**Authors:** {rec['authors']}\\n\\n\"\n",
    "            formatted_recommendations += f\"{rec['description']}\\n\"\n",
    "            formatted_recommendations += f\"[Access Paper]({rec['url']})\\n\\n\"\n",
    "        return formatted_recommendations\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating paper recommendations: {str(e)}\")\n",
    "        # Fallback to basic recommendations\n",
    "        key_terms = extract_key_terms(paper_content)\n",
    "        formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "        for i in range(1, 6):\n",
    "            term = key_terms[i % len(key_terms)] if key_terms else \"research\"\n",
    "            encoded_term = quote_plus(term)\n",
    "            formatted_recommendations += f\"## {i}. Recent advances in {term} research\\n\"\n",
    "            formatted_recommendations += f\"**Authors:** Various researchers\\n\\n\"\n",
    "            formatted_recommendations += f\"This paper explores {term} which is related to concepts in your research.\\n\"\n",
    "            formatted_recommendations += f\"[Access Paper](https://scholar.google.com/scholar?q={encoded_term})\\n\\n\"\n",
    "        return formatted_recommendations\n",
    "\n",
    "def create_full_paper_analysis(filename: str, summary_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report for paper analysis\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Paper Analysis: {filename}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "## Paper Summary\n",
    "\n",
    "{summary_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis was generated by AI Research Assistant using web crawling for recommendations*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "# --------------------- File and Display Functions ---------------------\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Convert a string to a valid filename\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "def save_markdown_file(topic: str, content: str) -> str:\n",
    "    \"\"\"Save content to a markdown file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_topic = sanitize_filename(topic)\n",
    "    filename = f\"research_{safe_topic}_{timestamp}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return filename\n",
    "\n",
    "def display_markdown(content: str, use_markdown_display: bool = True):\n",
    "    \"\"\"Display content as rendered markdown if in IPython environment\"\"\"\n",
    "    try:\n",
    "        if use_markdown_display:\n",
    "            display(Markdown(content))\n",
    "        else:\n",
    "            print(content)\n",
    "    except:\n",
    "        print(content)\n",
    "\n",
    "# --------------------- Paper Processing Functions ---------------------\n",
    "def process_research_paper(file_path: str, original_filename: Optional[str] = None, db_path: str = \"research_papers.db\") -> Dict[str, Any]:\n",
    "    \"\"\"Process a research paper file (PDF or DOCX)\"\"\"\n",
    "    if not original_filename:\n",
    "        original_filename = os.path.basename(file_path)\n",
    "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        text_content = extract_text_from_pdf(file_path)\n",
    "        file_type = 'pdf'\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        text_content = extract_text_from_docx(file_path)\n",
    "        file_type = 'docx'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=12000,\n",
    "        chunk_overlap=2000\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "    processing_text = chunks[0] if len(chunks) > 0 else text_content\n",
    "    \n",
    "    paper_id = save_file_to_database(original_filename, text_content, file_type, db_path=db_path)\n",
    "    \n",
    "    try:\n",
    "        print(\"- Generating research paper summary...\")\n",
    "        summary = paper_summary_chain.invoke(processing_text)\n",
    "        save_summary_to_database(paper_id, summary, db_path=db_path)\n",
    "        \n",
    "        print(\"- Finding related research papers using web crawling...\")\n",
    "        formatted_recommendations = generate_paper_recommendations_content(processing_text)\n",
    "        \n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"summary\": summary,\n",
    "            \"recommendations\": formatted_recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing research paper: {str(e)}\")\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# --------------------- Main Task Functions ---------------------\n",
    "def run_research(topic: str, use_markdown_display: bool = True, db_path: str = \"research_papers.db\") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Perform research on a specific topic\"\"\"\n",
    "    if not topic.strip():\n",
    "        print(\"Please enter a valid topic.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nResearching '{topic}'... This may take a moment.\")\n",
    "    \n",
    "    try:\n",
    "        initialize_database(db_path)\n",
    "        \n",
    "        print(\"- Generating detailed report...\")\n",
    "        report = generate_report(topic)\n",
    "        \n",
    "        print(\"- Finding related topics using web crawling...\")\n",
    "        recommendations = generate_recommendations(topic)\n",
    "        \n",
    "        full_report = create_full_report(topic, report, recommendations)\n",
    "        filename = save_markdown_file(topic, full_report)\n",
    "        \n",
    "        print(f\"\\nResearch completed! Report saved as {filename}\\n\")\n",
    "        \n",
    "        # Display the report if requested\n",
    "        if use_markdown_display:\n",
    "            display_markdown(full_report, use_markdown_display)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"filename\": filename,\n",
    "            \"report\": report,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing research: {str(e)}\")\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def analyze_research_paper(file_content, filename: str, use_markdown_display: bool = True, db_path: str = \"research_papers.db\") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Analyze a research paper from uploaded file content\"\"\"\n",
    "    if not file_content or not filename:\n",
    "        print(\"Please provide valid file content and filename.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nAnalyzing research paper '{filename}'... This may take a moment.\")\n",
    "    \n",
    "    try:\n",
    "        initialize_database(db_path)\n",
    "        \n",
    "        # Save file content to temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:\n",
    "            temp_file_path = temp_file.name\n",
    "            temp_file.write(file_content)\n",
    "        \n",
    "        result = process_research_paper(temp_file_path, filename, db_path)\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        try:\n",
    "            os.unlink(temp_file_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            full_analysis = create_full_paper_analysis(\n",
    "                filename, \n",
    "                result[\"summary\"], \n",
    "                result[\"recommendations\"]\n",
    "            )\n",
    "            \n",
    "            # Save analysis to markdown file\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            safe_filename = sanitize_filename(os.path.splitext(filename)[0])\n",
    "            output_filename = f\"analysis_{safe_filename}_{timestamp}.md\"\n",
    "            \n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(full_analysis)\n",
    "            \n",
    "            print(f\"\\nAnalysis completed! Report saved as {output_filename}\\n\")\n",
    "            \n",
    "            # Display the analysis if requested\n",
    "            if use_markdown_display:\n",
    "                display_markdown(full_analysis, use_markdown_display)\n",
    "            \n",
    "            result[\"output_filename\"] = output_filename\n",
    "            result[\"full_analysis\"] = full_analysis\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing research paper: {str(e)}\")\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "def analyze_paper_by_id(paper_id: int, use_markdown_display: bool = True, db_path: str = \"research_papers.db\") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Analyze a research paper by its ID in the database\"\"\"\n",
    "    try:\n",
    "        paper_data = get_paper_from_database(paper_id, db_path)\n",
    "        if not paper_data:\n",
    "            print(f\"No paper found with ID {paper_id}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nAnalyzing research paper ID {paper_id}... This may take a moment.\")\n",
    "        \n",
    "        # Generate summary if not already present\n",
    "        if not paper_data.get(\"summary\"):\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=12000,\n",
    "                chunk_overlap=2000\n",
    "            )\n",
    "            chunks = text_splitter.split_text(paper_data[\"content\"])\n",
    "            processing_text = chunks[0] if len(chunks) > 0 else paper_data[\"content\"]\n",
    "            \n",
    "            print(\"- Generating research paper summary...\")\n",
    "            summary = paper_summary_chain.invoke(processing_text)\n",
    "            save_summary_to_database(paper_id, summary, db_path)\n",
    "            paper_data[\"summary\"] = summary\n",
    "        \n",
    "        print(\"- Finding related research papers using web crawling...\")\n",
    "        recommendations = generate_paper_recommendations_content(paper_data[\"content\"])\n",
    "        \n",
    "        full_analysis = create_full_paper_analysis(\n",
    "            paper_data[\"filename\"], \n",
    "            paper_data[\"summary\"], \n",
    "            recommendations\n",
    "        )\n",
    "        \n",
    "        # Save analysis to markdown file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        safe_filename = sanitize_filename(os.path.splitext(paper_data[\"filename\"])[0])\n",
    "        output_filename = f\"analysis_{safe_filename}_{timestamp}.md\"\n",
    "        \n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_analysis)\n",
    "        \n",
    "        print(f\"\\nAnalysis completed! Report saved as {output_filename}\\n\")\n",
    "        \n",
    "        # Display the analysis if requested\n",
    "        if use_markdown_display:\n",
    "            display_markdown(full_analysis, use_markdown_display)\n",
    "        \n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": paper_data[\"filename\"],\n",
    "            \"summary\": paper_data[\"summary\"],\n",
    "            \"recommendations\": recommendations,\n",
    "            \"output_filename\": output_filename,\n",
    "            \"full_analysis\": full_analysis,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing paper from database: {str(e)}\")\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# --------------------- File Upload Helpers ---------------------\n",
    "def encode_file_to_base64(file_path: str) -> str:\n",
    "    \"\"\"Encode file content to base64 for display\"\"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        file_content = file.read()\n",
    "        return base64.b64encode(file_content).decode('utf-8')\n",
    "\n",
    "def process_base64_file(base64_data: str, filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process file from base64 encoded data\"\"\"\n",
    "    try:\n",
    "        # Extract content type and actual base64 data if it's a data URL\n",
    "        if base64_data.startswith('data:'):\n",
    "            format_spec, base64_data = base64_data.split(';base64,', 1)\n",
    "        \n",
    "        # Decode base64 data\n",
    "        file_content = base64.b64decode(base64_data)\n",
    "        \n",
    "        # Save to temporary file and process\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:\n",
    "            temp_file_path = temp_file.name\n",
    "            temp_file.write(file_content)\n",
    "        \n",
    "        result = process_research_paper(temp_file_path, filename)\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        try:\n",
    "            os.unlink(temp_file_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing base64 file: {str(e)}\")\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "# --------------------- HTML Display Functions ---------------------\n",
    "def get_html_report_display(report_content: str) -> str:\n",
    "    \"\"\"Convert markdown report to HTML for display\"\"\"\n",
    "    try:\n",
    "        from markdown import markdown\n",
    "        html_content = markdown(report_content)\n",
    "        return f\"\"\"\n",
    "        <div style=\"font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; line-height: 1.6;\">\n",
    "            {html_content}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    except:\n",
    "        # Simple fallback conversion if markdown module is not available\n",
    "        html_content = report_content.replace(\"\\n\\n\", \"<br><br>\")\n",
    "        html_content = html_content.replace(\"# \", \"<h1>\").replace(\"\\n## \", \"</h1><h2>\")\n",
    "        html_content = html_content.replace(\"## \", \"<h2>\").replace(\"\\n### \", \"</h2><h3>\")\n",
    "        html_content = html_content.replace(\"### \", \"<h3>\").replace(\"\\n#### \", \"</h3><h4>\")\n",
    "        html_content = html_content.replace(\"**\", \"<strong>\").replace(\"**\", \"</strong>\")\n",
    "        html_content = html_content.replace(\"*\", \"<em>\").replace(\"*\", \"</em>\")\n",
    "        return f\"\"\"\n",
    "        <div style=\"font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; line-height: 1.6;\">\n",
    "            {html_content}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "def display_html_report(report_content: str):\n",
    "    \"\"\"Display report as HTML in IPython environment\"\"\"\n",
    "    try:\n",
    "        html_content = get_html_report_display(report_content)\n",
    "        display(HTML(html_content))\n",
    "    except:\n",
    "        print(report_content)\n",
    "\n",
    "# --------------------- Command Line Interface ---------------------\n",
    "def main():\n",
    "    \"\"\"Command line interface for the research assistant\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"AI Research Assistant\")\n",
    "    subparsers = parser.add_subparsers(dest=\"command\", help=\"Command to run\")\n",
    "    \n",
    "    # Research topic command\n",
    "    research_parser = subparsers.add_parser(\"research\", help=\"Research a topic\")\n",
    "    research_parser.add_argument(\"topic\", help=\"Topic to research\")\n",
    "    \n",
    "    # Analyze paper command\n",
    "    analyze_parser = subparsers.add_parser(\"analyze\", help=\"Analyze a research paper\")\n",
    "    analyze_parser.add_argument(\"file_path\", help=\"Path to research paper file (PDF or DOCX)\")\n",
    "    \n",
    "    # Process database entry command\n",
    "    db_parser = subparsers.add_parser(\"paper\", help=\"Process a paper from the database\")\n",
    "    db_parser.add_argument(\"paper_id\", type=int, help=\"ID of the paper in the database\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.command == \"research\":\n",
    "        run_research(args.topic, use_markdown_display=False)\n",
    "    elif args.command == \"analyze\":\n",
    "        with open(args.file_path, \"rb\") as file:\n",
    "            file_content = file.read()\n",
    "            analyze_research_paper(file_content, os.path.basename(args.file_path), use_markdown_display=False)\n",
    "    elif args.command == \"paper\":\n",
    "        analyze_paper_by_id(args.paper_id, use_markdown_display=False)\n",
    "    else:\n",
    "        parser.print_help()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fahim\\miniconda3\\envs\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Research Assistant\n",
      "--------------------\n",
      "\n",
      "Options:\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Exit\n",
      "- Generating research paper summary...\n",
      "- Finding related research papers via web search...\n",
      "Paper analysis saved to: research_s42979-021-00815-1_20250322_033038.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Research Paper Analysis: s42979-021-00815-1.pdf\n",
       "\n",
       "*Generated on: 2025-03-22 03:30:38*\n",
       "\n",
       "---\n",
       "\n",
       "## Paper Summary\n",
       "\n",
       "# Deep Learning: A Comprehensive Overview - Summary\n",
       "\n",
       "This paper provides a comprehensive overview of deep learning (DL) techniques, their taxonomy, applications, and potential research directions. It aims to serve as a reference guide for both academia and industry professionals interested in developing data-driven smart and intelligent systems based on DL.\n",
       "\n",
       "## 1. Main Objective\n",
       "\n",
       "The main objective is to present a structured and comprehensive view of DL techniques, addressing the challenges of building appropriate DL models due to the dynamic nature of real-world problems and the \"black-box\" nature of DL models. The paper aims to provide a clear understanding of DL by categorizing techniques and highlighting their applications.\n",
       "\n",
       "## 2. Methodology\n",
       "\n",
       "The paper employs a review-based methodology, exploring various DL techniques and categorizing them into a taxonomy. This taxonomy considers three major categories:\n",
       "\n",
       "*   Deep networks for supervised/discriminative learning.\n",
       "*   Deep networks for unsupervised/generative learning.\n",
       "*   Deep networks for hybrid learning.\n",
       "\n",
       "The paper also summarizes real-world application areas of DL. Furthermore, it identifies potential aspects and research directions for future generation DL modeling.\n",
       "\n",
       "## 3. Key Findings and Results\n",
       "\n",
       "*   DL is a core technology of the Fourth Industrial Revolution (Industry 4.0) due to its learning capabilities from data.\n",
       "*   DL techniques can be categorized into supervised, unsupervised, and hybrid learning approaches.\n",
       "*   DL has numerous applications in areas like healthcare, visual recognition, text analytics, and cybersecurity.\n",
       "*   The paper identifies ten potential aspects for future generation DL modeling with research directions.\n",
       "\n",
       "## 4. Main Conclusions and Implications\n",
       "\n",
       "The paper concludes that DL is a powerful tool for creating intelligent systems. The provided taxonomy and overview of applications can help researchers and practitioners better understand and apply DL techniques. The identified research directions can guide future development in the field.\n",
       "\n",
       "## 5. Limitations\n",
       "\n",
       "The paper implicitly acknowledges a limitation in that DL models are often considered \"black-box\" machines, hindering standard development. The paper aims to address this limitation by providing a structured overview of DL techniques.\n",
       "\n",
       "---\n",
       "\n",
       "# Related Research Papers You May Be Interested In\n",
       "\n",
       "## 1. Towards Robust Classification: A Hybrid Deep Learning Framework Combining Discriminative and Generative Models (2023)\n",
       "**Authors:** Li, Zhang et al.\n",
       "\n",
       "This paper relates to your research topic and expands on the concepts in your paper.\n",
       "[Access Paper](https://arxiv.org/abs/2303.12345)\n",
       "\n",
       "## 2. Adversarial Training for Enhanced Discriminative Learning in Deep Neural Networks (2019)\n",
       "**Authors:** Kim, Park et al.\n",
       "\n",
       "This paper relates to your research topic and expands on the concepts in your paper.\n",
       "[Access Paper](https://proceedings.mlr.press/v97/kim19a.html)\n",
       "\n",
       "## 3. Deep Generative Models for Semi-Supervised Learning: A Comparative Study of VAEs and GANs (2018)\n",
       "**Authors:** Gupta, Sharma et al.\n",
       "\n",
       "This paper relates to your research topic and expands on the concepts in your paper.\n",
       "[Access Paper](https://openreview.net/forum?id=Hkpo_vhyz)\n",
       "\n",
       "## 4. Hybrid Learning Strategies for Efficient Training of Deep Neural Networks on Resource-Constrained Devices (2021)\n",
       "**Authors:** Silva, Rodriguez et al.\n",
       "\n",
       "This paper relates to your research topic and expands on the concepts in your paper.\n",
       "[Access Paper](https://ieeexplore.ieee.org/document/9543210)\n",
       "\n",
       "## 5. Bridging the Gap Between Discriminative and Generative Approaches in Deep Representation Learning (2017)\n",
       "**Authors:** Chen, Wang et al.\n",
       "\n",
       "This paper relates to your research topic and expands on the concepts in your paper.\n",
       "[Access Paper](https://www.jmlr.org/papers/volume18/16-594/16-594.pdf)\n",
       "\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "*This analysis was generated by AI Research Assistant*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import PyPDF2 \n",
    "import docx  \n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_environment():\n",
    "    \"\"\"Load environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "    return os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "def initialize_model(api_key):\n",
    "    \"\"\"Initialize Gemini model\"\"\"\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "\n",
    "def define_output_schemas():\n",
    "    \"\"\"Define output schemas for topic and paper recommendations\"\"\"\n",
    "    # Output schema for topic recommendations\n",
    "    class RecommendedTopic(BaseModel):\n",
    "        topic: str = Field(description=\"The name of the recommended topic\")\n",
    "        description: str = Field(description=\"A brief description of why this topic is relevant\")\n",
    "        resource_url: str = Field(description=\"A relevant resource URL for this topic\")\n",
    "\n",
    "    class TopicRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedTopic] = Field(description=\"List of recommended related topics\")\n",
    "\n",
    "    # Output schema for paper recommendations\n",
    "    class RecommendedPaper(BaseModel):\n",
    "        title: str = Field(description=\"The title of the recommended research paper\")\n",
    "        authors: str = Field(description=\"The authors of the paper\")\n",
    "        year: str = Field(description=\"Publication year\")\n",
    "        description: str = Field(description=\"Brief description of relevance to the original paper\")\n",
    "        paper_url: str = Field(description=\"URL to access this paper\", default=\"\")\n",
    "\n",
    "    class PaperRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedPaper] = Field(description=\"List of recommended related papers\")\n",
    "        \n",
    "    return TopicRecommendations, PaperRecommendations\n",
    "\n",
    "def create_prompt_templates():\n",
    "    \"\"\"Create prompt templates for research tasks\"\"\"\n",
    "    report_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a comprehensive, detailed report on the following topic:\n",
    "        \n",
    "        Topic: {topic}\n",
    "        \n",
    "        Your report should include:\n",
    "        1. Introduction to the topic\n",
    "        2. Key concepts and definitions\n",
    "        3. Historical context and development\n",
    "        4. Current state and applications\n",
    "        5. Future directions and potential developments\n",
    "        6. Conclusion\n",
    "        \n",
    "        Format your report with clear markdown headings and subheadings. Use proper markdown formatting for emphasis, lists, and other elements.\n",
    "        Make sure to provide in-depth analysis.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Based on the topic: {topic}\n",
    "        \n",
    "        Generate 5 relevant related topics that the user might be interested in researching next.\n",
    "        For each recommendation, provide:\n",
    "        1. The topic name\n",
    "        2. A brief 1-2 sentence description of why it's relevant\n",
    "        3. A relevant resource URL that would contain valuable information about this topic\n",
    "        \n",
    "        Your response must be formatted as a valid JSON object that matches this structure:\n",
    "        {\n",
    "            \"recommendations\": [\n",
    "                {\n",
    "                    \"topic\": \"Topic Name\",\n",
    "                    \"description\": \"Brief description of relevance\",\n",
    "                    \"resource_url\": \"https://example.com/relevant-page\"\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        Use reputable sources for your resource URLs. While you can't verify if the exact URLs exist,\n",
    "        make them realistic and likely to contain quality information.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a concise but comprehensive summary of the following research paper:\n",
    "        \n",
    "        Paper content: {paper_content}\n",
    "        \n",
    "        Your summary should include:\n",
    "        1. Main objective of the research\n",
    "        2. Methodology used\n",
    "        3. Key findings and results\n",
    "        4. Main conclusions and implications\n",
    "        5. Limitations (if mentioned)\n",
    "        \n",
    "        Format your summary with clear markdown headings and keep it concise yet informative.\n",
    "        Focus on the most important aspects of the paper.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Based on the following research paper:\n",
    "    \n",
    "    Paper content: {paper_content}\n",
    "    \n",
    "    Generate 5 relevant related research papers that the user might be interested in reading next.\n",
    "    These should be real papers that likely exist in the academic literature.\n",
    "    \n",
    "    For each recommendation, provide:\n",
    "    1. The paper title (use the actual title of a real paper if you know it)\n",
    "    2. The authors (use \"et al.\" for multiple authors after the first)\n",
    "    3. Publication year (estimate if necessary)\n",
    "    4. A brief description of why it's relevant to the original paper\n",
    "    5. A URL where the paper might be found - THIS IS CRITICAL. \n",
    "    \n",
    "    For URLs, use specific links from:\n",
    "    - Google Scholar (https://scholar.google.com/scholar?q=PAPER_TITLE)\n",
    "    - arXiv (https://arxiv.org/search/?query=PAPER_TITLE)\n",
    "    - ResearchGate (https://www.researchgate.net/search.Search.html?query=PAPER_TITLE)\n",
    "    - ACM Digital Library (https://dl.acm.org/action/doSearch?AllField=PAPER_TITLE)\n",
    "    - IEEE Xplore (https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=PAPER_TITLE)\n",
    "    \n",
    "    Replace PAPER_TITLE with URL-encoded paper title in these templates. Make sure EVERY recommendation has a working URL.\n",
    "    \n",
    "    Your response must be formatted as a valid JSON object that matches this structure:\n",
    "    {{\n",
    "        \"recommendations\": [\n",
    "            {{\n",
    "                \"title\": \"Paper Title\",\n",
    "                \"authors\": \"Author names\",\n",
    "                \"year\": \"Publication year\",\n",
    "                \"description\": \"Brief description of relevance\",\n",
    "                \"paper_url\": \"https://example.com/paper-link\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "    \n",
    "    return report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt\n",
    "\n",
    "def create_chains(model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt, TopicRecommendations, PaperRecommendations):\n",
    "    \"\"\"Create processing chains for research tasks\"\"\"\n",
    "    report_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | report_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    recommendation_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=TopicRecommendations)\n",
    "    )\n",
    "\n",
    "    paper_summary_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_summary_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    paper_recommendation_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=PaperRecommendations)\n",
    "    )\n",
    "    \n",
    "    return report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain\n",
    "\n",
    "def initialize_database(db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Initialize SQLite database for storing papers\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            content TEXT NOT NULL,\n",
    "            file_type TEXT NOT NULL,\n",
    "            upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            summary TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_file_to_database(filename: str, content: str, file_type: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save file content to SQLite database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO papers (filename, content, file_type) VALUES (?, ?, ?)\",\n",
    "        (filename, content, file_type)\n",
    "    )\n",
    "    paper_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return paper_id\n",
    "\n",
    "def save_summary_to_database(paper_id: int, summary: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save paper summary to database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"UPDATE papers SET summary = ? WHERE id = ?\",\n",
    "        (summary, paper_id)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_paper_from_database(paper_id: int, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Retrieve paper content from database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT filename, content, file_type, summary FROM papers WHERE id = ?\", (paper_id,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    if result:\n",
    "        return {\n",
    "            \"filename\": result[0],\n",
    "            \"content\": result[1],\n",
    "            \"file_type\": result[2],\n",
    "            \"summary\": result[3]\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        # Fallback method\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a DOCX file\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from DOCX: {str(e)}\")\n",
    "        # Fallback method\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def generate_report(topic: str, report_chain) -> str:\n",
    "    \"\"\"Generate a detailed report on the given topic\"\"\"\n",
    "    return report_chain.invoke(topic)\n",
    "\n",
    "def search_web(query, num_results=5):\n",
    "    \"\"\"Search the web for related topics\"\"\"\n",
    "    try:\n",
    "        # Format query for search engines\n",
    "        search_query = urllib.parse.quote_plus(query)\n",
    "        \n",
    "        # List of search URLs to try\n",
    "        search_urls = [\n",
    "            f\"https://www.google.com/search?q={search_query}\",\n",
    "            f\"https://en.wikipedia.org/wiki/Special:Search?search={search_query}&go=Go\",\n",
    "            f\"https://scholar.google.com/scholar?q={search_query}\"\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Set user agent to avoid being blocked\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Try each search engine until we get enough results\n",
    "        for search_url in search_urls:\n",
    "            if len(results) >= num_results:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                response = requests.get(search_url, headers=headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # Extract links and titles (implementation varies by search engine)\n",
    "                    if 'google.com/search' in search_url:\n",
    "                        # For Google\n",
    "                        search_results = soup.select('div.g')\n",
    "                        for result in search_results:\n",
    "                            title_element = result.select_one('h3')\n",
    "                            link_element = result.select_one('a')\n",
    "                            \n",
    "                            if title_element and link_element and 'href' in link_element.attrs:\n",
    "                                title = title_element.get_text()\n",
    "                                link = link_element['href']\n",
    "                                \n",
    "                                # Remove Google redirects\n",
    "                                if link.startswith('/url?q='):\n",
    "                                    link = link.split('/url?q=')[1].split('&')[0]\n",
    "                                \n",
    "                                if link.startswith('http') and not any(x['url'] == link for x in results):\n",
    "                                    results.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link\n",
    "                                    })\n",
    "                                    \n",
    "                                if len(results) >= num_results:\n",
    "                                    break\n",
    "                    \n",
    "                    elif 'wikipedia.org' in search_url:\n",
    "                        # For Wikipedia\n",
    "                        search_results = soup.select('ul.mw-search-results li')\n",
    "                        for result in search_results:\n",
    "                            title_element = result.select_one('a')\n",
    "                            if title_element and 'href' in title_element.attrs:\n",
    "                                title = title_element.get_text()\n",
    "                                link = 'https://en.wikipedia.org' + title_element['href']\n",
    "                                \n",
    "                                if not any(x['url'] == link for x in results):\n",
    "                                    results.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link\n",
    "                                    })\n",
    "                                    \n",
    "                                if len(results) >= num_results:\n",
    "                                    break\n",
    "                    \n",
    "                    elif 'scholar.google.com' in search_url:\n",
    "                        # For Google Scholar\n",
    "                        search_results = soup.select('div.gs_ri')\n",
    "                        for result in search_results:\n",
    "                            title_element = result.select_one('h3 a')\n",
    "                            if title_element and 'href' in title_element.attrs:\n",
    "                                title = title_element.get_text()\n",
    "                                link = title_element['href']\n",
    "                                \n",
    "                                if not link.startswith('http'):\n",
    "                                    link = 'https://scholar.google.com' + link\n",
    "                                \n",
    "                                if not any(x['url'] == link for x in results):\n",
    "                                    results.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link\n",
    "                                    })\n",
    "                                    \n",
    "                                if len(results) >= num_results:\n",
    "                                    break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error searching {search_url}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Web search error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def generate_recommendations(topic: str, recommendation_chain=None, model=None) -> str:\n",
    "    \"\"\"Generate relevant topic recommendations using web crawling\"\"\"\n",
    "    try:\n",
    "        print(\"- Searching the web for related topics...\")\n",
    "        search_results = search_web(topic, num_results=15)\n",
    "        \n",
    "        # If we didn't get enough results, try searching for \"related to [topic]\"\n",
    "        if len(search_results) < 5:\n",
    "            additional_results = search_web(f\"related to {topic}\", num_results=10)\n",
    "            for result in additional_results:\n",
    "                if result not in search_results:\n",
    "                    search_results.append(result)\n",
    "        \n",
    "        # Extract relevant topics from search results\n",
    "        formatted_recommendations = \"# Related Topics You May Be Interested In\\n\\n\"\n",
    "        used_topics = set()\n",
    "        count = 0\n",
    "        \n",
    "        for result in search_results:\n",
    "            if count >= 5:\n",
    "                break\n",
    "                \n",
    "            # Extract a topic name from the search result title\n",
    "            title = result['title']\n",
    "            url = result['url']\n",
    "            \n",
    "            # Skip if URL is suspicious\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                continue\n",
    "                \n",
    "            # Generate a clean topic name\n",
    "            topic_name = re.sub(r'\\s*\\|.*$', '', title)  # Remove text after pipe symbol\n",
    "            topic_name = re.sub(r'\\s*-.*$', '', topic_name)  # Remove text after dash\n",
    "            \n",
    "            # Skip very short or very long topic names\n",
    "            if len(topic_name) < 5 or len(topic_name) > 100:\n",
    "                continue\n",
    "                \n",
    "            # Skip if too similar to original topic\n",
    "            if topic_name.lower() == topic.lower():\n",
    "                continue\n",
    "                \n",
    "            # Check if we've already used a similar topic\n",
    "            similar = False\n",
    "            for used_topic in used_topics:\n",
    "                if (topic_name.lower() in used_topic.lower() or \n",
    "                    used_topic.lower() in topic_name.lower()):\n",
    "                    similar = True\n",
    "                    break\n",
    "            \n",
    "            if similar:\n",
    "                continue\n",
    "                \n",
    "            used_topics.add(topic_name)\n",
    "            count += 1\n",
    "            \n",
    "            # Generate a description using the topic and original query\n",
    "            description = f\"This topic is closely related to {topic} and offers additional perspectives and insights.\"\n",
    "            \n",
    "            formatted_recommendations += f\"## {count}. {topic_name}\\n\"\n",
    "            formatted_recommendations += f\"{description}\\n\"\n",
    "            formatted_recommendations += f\"[Learn more]({url})\\n\\n\"\n",
    "        \n",
    "        # If we didn't get enough recommendations from web search, generate the missing ones\n",
    "        if count < 5:\n",
    "            # Create a backup prompt for generating the remaining recommendations\n",
    "            backup_prompt = ChatPromptTemplate.from_template(\n",
    "                f\"\"\"\n",
    "                I already have the following related topics for \"{topic}\":\n",
    "                {\", \".join(used_topics)}\n",
    "                \n",
    "                Please suggest {5 - count} more related topics that are different from the ones above.\n",
    "                For each topic, provide:\n",
    "                1. The topic name\n",
    "                2. A brief description of why it's relevant to {topic}\n",
    "                3. A reasonable URL where someone might learn about this topic (like a Wikipedia or educational site)\n",
    "                \n",
    "                Format each recommendation like this:\n",
    "                Topic: [topic name]\n",
    "                Description: [description]\n",
    "                URL: [url]\n",
    "                \"\"\"\n",
    "            )\n",
    "            backup_chain = backup_prompt | model | StrOutputParser()\n",
    "            additional_recs = backup_chain.invoke({})\n",
    "            \n",
    "            # Parse the generated recommendations\n",
    "            for line in additional_recs.split(\"\\n\\n\"):\n",
    "                if count >= 5:\n",
    "                    break\n",
    "                    \n",
    "                match_topic = re.search(r\"Topic:(.+)\", line)\n",
    "                match_desc = re.search(r\"Description:(.+)\", line)\n",
    "                match_url = re.search(r\"URL:(.+)\", line)\n",
    "                \n",
    "                if match_topic and match_desc and match_url:\n",
    "                    topic_name = match_topic.group(1).strip()\n",
    "                    description = match_desc.group(1).strip()\n",
    "                    url = match_url.group(1).strip()\n",
    "                    \n",
    "                    count += 1\n",
    "                    formatted_recommendations += f\"## {count}. {topic_name}\\n\"\n",
    "                    formatted_recommendations += f\"{description}\\n\"\n",
    "                    formatted_recommendations += f\"[Learn more]({url})\\n\\n\"\n",
    "        \n",
    "        return formatted_recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in web-based recommendations: {str(e)}\")\n",
    "        # Fallback to using the LLM directly if web search fails\n",
    "        backup_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Based on the topic: {topic}\n",
    "            \n",
    "            Provide 5 relevant related topics that the user might be interested in researching next.\n",
    "            For each recommendation, provide:\n",
    "            1. The topic name\n",
    "            2. A brief description of why it's relevant\n",
    "            3. A relevant resource link\n",
    "            \n",
    "            Format your response as a markdown list.\n",
    "            \"\"\"\n",
    "        )\n",
    "        backup_chain = backup_prompt | model | StrOutputParser()\n",
    "        return backup_chain.invoke({\"topic\": topic})\n",
    "\n",
    "def generate_paper_recommendations(paper_content, paper_recommendation_chain=None, model=None) -> str:\n",
    "    \"\"\"Generate recommendations for related papers using web crawling\"\"\"\n",
    "    try:\n",
    "        # Extract key phrases from the paper\n",
    "        key_phrases_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Extract 5 key technical phrases or terms from the following paper that could be used to find related research.\n",
    "            Only return the phrases as a comma-separated list with no additional text.\n",
    "            \n",
    "            Paper content: {paper_content}\n",
    "            \"\"\"\n",
    "        )\n",
    "        key_phrases_chain = key_phrases_prompt | model | StrOutputParser()\n",
    "        key_phrases = key_phrases_chain.invoke({\"paper_content\": paper_content}).split(\",\")\n",
    "        \n",
    "        # Search for related papers using the key phrases\n",
    "        papers = []\n",
    "        for phrase in key_phrases:\n",
    "            if len(papers) >= 10:\n",
    "                break\n",
    "                \n",
    "            search_results = search_web(f\"{phrase.strip()} research paper\", num_results=5)\n",
    "            for result in search_results:\n",
    "                if len(papers) >= 10:\n",
    "                    break\n",
    "                \n",
    "                title = result['title']\n",
    "                url = result['url']\n",
    "                \n",
    "                # Skip non-academic-looking results\n",
    "                if not any(domain in url for domain in ['.edu', 'arxiv.org', 'scholar.google', 'researchgate', 'ieee.org', 'acm.org']):\n",
    "                    continue\n",
    "                    \n",
    "                # Skip very short titles\n",
    "                if len(title) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if we already have this paper\n",
    "                if any(p['title'] == title for p in papers):\n",
    "                    continue\n",
    "                    \n",
    "                # Add the paper to our results\n",
    "                papers.append({\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'phrase': phrase.strip()\n",
    "                })\n",
    "        \n",
    "        # Process the best 5 results\n",
    "        top_papers = papers[:5]\n",
    "        \n",
    "        # If we don't have enough papers, generate some with the model\n",
    "        if len(top_papers) < 5:\n",
    "            remaining = 5 - len(top_papers)\n",
    "            paper_gen_prompt = ChatPromptTemplate.from_template(\n",
    "                f\"\"\"\n",
    "                Based on the key phrases {', '.join(key_phrases)}, \n",
    "                suggest {remaining} academic papers that would be related to a paper discussing these topics.\n",
    "                \n",
    "                For each paper, provide:\n",
    "                1. A realistic paper title\n",
    "                2. Author names (use et al. for multiple authors)\n",
    "                3. A realistic publication year (between 2015-2024)\n",
    "                4. A URL where the paper might be found\n",
    "                \n",
    "                Format each paper like this:\n",
    "                Title: [title]\n",
    "                Authors: [authors]\n",
    "                Year: [year]\n",
    "                URL: [url]\n",
    "                \"\"\"\n",
    "            )\n",
    "            paper_gen_chain = paper_gen_prompt | model | StrOutputParser()\n",
    "            additional_papers = paper_gen_chain.invoke({})\n",
    "            \n",
    "            # Parse the generated papers\n",
    "            current_paper = {}\n",
    "            for line in additional_papers.split('\\n'):\n",
    "                if line.startswith('Title:'):\n",
    "                    if current_paper and 'title' in current_paper:\n",
    "                        top_papers.append(current_paper)\n",
    "                        current_paper = {}\n",
    "                    current_paper['title'] = line.replace('Title:', '').strip()\n",
    "                elif line.startswith('Authors:'):\n",
    "                    current_paper['authors'] = line.replace('Authors:', '').strip()\n",
    "                elif line.startswith('Year:'):\n",
    "                    current_paper['year'] = line.replace('Year:', '').strip()\n",
    "                elif line.startswith('URL:'):\n",
    "                    current_paper['url'] = line.replace('URL:', '').strip()\n",
    "            \n",
    "            if current_paper and 'title' in current_paper:\n",
    "                top_papers.append(current_paper)\n",
    "        \n",
    "        # Generate descriptions for each paper\n",
    "        formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "        for i, paper in enumerate(top_papers[:5], 1):\n",
    "            title = paper.get('title', '')\n",
    "            url = paper.get('url', '')\n",
    "            \n",
    "            # Generate paper metadata if missing\n",
    "            authors = paper.get('authors', 'Various authors')\n",
    "            if 'authors' not in paper:\n",
    "                # Extract authors from title or URL if possible, otherwise use placeholder\n",
    "                authors = \"Various authors\"\n",
    "                \n",
    "            year = paper.get('year', '2023')\n",
    "            if 'year' not in paper:\n",
    "                # Try to extract year from URL or title, otherwise use recent year\n",
    "                year_match = re.search(r'20[12]\\d', title)\n",
    "                if year_match:\n",
    "                    year = year_match.group(0)\n",
    "                else:\n",
    "                    year = \"2023\"\n",
    "            \n",
    "            # Generate a description based on the title and the original key phrase\n",
    "            description = f\"This paper relates to {paper.get('phrase', 'your research topic')} and expands on the concepts in your paper.\"\n",
    "            \n",
    "            # Add the recommendation\n",
    "            formatted_recommendations += f\"## {i}. {title} ({year})\\n\"\n",
    "            formatted_recommendations += f\"**Authors:** {authors}\\n\\n\"\n",
    "            formatted_recommendations += f\"{description}\\n\"\n",
    "            formatted_recommendations += f\"[Access Paper]({url})\\n\\n\"\n",
    "            \n",
    "        return formatted_recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in web-based paper recommendations: {str(e)}\")\n",
    "        # Fallback to using the paper recommendation chain\n",
    "        try:\n",
    "            recommendations_data = paper_recommendation_chain.invoke(paper_content)\n",
    "            recs = recommendations_data[\"recommendations\"]  # Access as a dict\n",
    "            formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "            for i, rec in enumerate(recs, 1):\n",
    "                formatted_recommendations += f\"## {i}. {rec['title']} ({rec['year']})\\n\"\n",
    "                formatted_recommendations += f\"**Authors:** {rec['authors']}\\n\\n\"\n",
    "                formatted_recommendations += f\"{rec['description']}\\n\"\n",
    "                paper_url = rec['paper_url'].strip()\n",
    "                if not paper_url:\n",
    "                    encoded_title = re.sub(r'\\s+', '+', rec['title'])\n",
    "                    paper_url = f\"https://scholar.google.com/scholar?q={encoded_title}\"\n",
    "                formatted_recommendations += f\"[Access Paper]({paper_url})\\n\\n\"\n",
    "            return formatted_recommendations\n",
    "        except Exception as e:\n",
    "            # If everything fails, use a simple backup prompt\n",
    "            backup_prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                Based on the following research paper content:\n",
    "                \n",
    "                {paper_content}\n",
    "                \n",
    "                Provide 5 relevant related research papers that might be of interest.\n",
    "                For each paper, include:\n",
    "                1. Title (a real paper title if possible)\n",
    "                2. Authors\n",
    "                3. Year\n",
    "                4. Brief description of relevance\n",
    "                5. A direct URL to access the paper\n",
    "                \n",
    "                Format your response in markdown with clear headings and clickable links.\n",
    "                \"\"\"\n",
    "            )\n",
    "            backup_chain = backup_prompt | model | StrOutputParser()\n",
    "            return backup_chain.invoke({\"paper_content\": paper_content})\n",
    "\n",
    "def create_full_report(topic: str, report_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report combining the report and recommendations\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Report: {topic}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "{report_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This report was generated by AI Research Assistant*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def create_full_paper_analysis(filename: str, summary_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report for paper analysis\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Paper Analysis: {filename}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "## Paper Summary\n",
    "\n",
    "{summary_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis was generated by AI Research Assistant*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Convert a string to a valid filename\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "def save_markdown_file(topic: str, content: str) -> str:\n",
    "    \"\"\"Save content to a markdown file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_topic = sanitize_filename(topic)\n",
    "    filename = f\"research_{safe_topic}_{timestamp}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return filename\n",
    "\n",
    "def display_markdown(content: str, use_markdown_display: bool = True):\n",
    "    \"\"\"Display content as rendered markdown if in IPython environment\"\"\"\n",
    "    try:\n",
    "        if use_markdown_display:\n",
    "            display(Markdown(content))\n",
    "        else:\n",
    "            print(content)\n",
    "    except:\n",
    "        print(content)\n",
    "\n",
    "def process_research_paper(file_path: str, original_filename: Optional[str] = None, \n",
    "                           db_path: str = \"../data/research_papers.db\", \n",
    "                           paper_summary_chain=None, \n",
    "                           paper_recommendation_chain=None, \n",
    "                           model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Process a research paper file (PDF or DOCX)\"\"\"\n",
    "    if not original_filename:\n",
    "        original_filename = os.path.basename(file_path)\n",
    "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        text_content = extract_text_from_pdf(file_path)\n",
    "        file_type = 'pdf'\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        text_content = extract_text_from_docx(file_path)\n",
    "        file_type = 'docx'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=12000,\n",
    "        chunk_overlap=2000\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "    processing_text = chunks[0] if len(chunks) > 0 else text_content\n",
    "    \n",
    "    paper_id = save_file_to_database(original_filename, text_content, file_type, db_path=db_path)\n",
    "    \n",
    "    try:\n",
    "        print(\"- Generating research paper summary...\")\n",
    "        summary = paper_summary_chain.invoke(processing_text)\n",
    "        save_summary_to_database(paper_id, summary, db_path=db_path)\n",
    "        \n",
    "        print(\"- Finding related research papers via web search...\")\n",
    "        formatted_recommendations = generate_paper_recommendations(processing_text, paper_recommendation_chain, model)\n",
    "        \n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"summary\": summary,\n",
    "            \"recommendations\": formatted_recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing research paper: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def upload_research_paper_file(file_data, filename: str, \n",
    "                               paper_summary_chain=None, \n",
    "                               paper_recommendation_chain=None, \n",
    "                               model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Process an uploaded research paper file\"\"\"\n",
    "    try:\n",
    "        # Create a temporary file to work with\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:\n",
    "            temp_file.write(file_data)\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        # Process the paper\n",
    "        result = process_research_paper(\n",
    "            temp_path, \n",
    "            original_filename=filename,\n",
    "            paper_summary_chain=paper_summary_chain,\n",
    "            paper_recommendation_chain=paper_recommendation_chain,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_path)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            # Create full analysis report\n",
    "            full_analysis = create_full_paper_analysis(\n",
    "                filename,\n",
    "                result[\"summary\"],\n",
    "                result[\"recommendations\"]\n",
    "            )\n",
    "            \n",
    "            # Save the full analysis to a file\n",
    "            report_filename = save_markdown_file(\n",
    "                os.path.splitext(filename)[0],\n",
    "                full_analysis\n",
    "            )\n",
    "            \n",
    "            result[\"report_filename\"] = report_filename\n",
    "            result[\"full_analysis\"] = full_analysis\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Error processing uploaded file: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def research_topic(topic: str, report_chain=None, recommendation_chain=None, model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Conduct research on a given topic\"\"\"\n",
    "    try:\n",
    "        print(f\"Researching topic: {topic}\")\n",
    "        \n",
    "        print(\"- Generating detailed report...\")\n",
    "        report_content = generate_report(topic, report_chain)\n",
    "        \n",
    "        print(\"- Finding related topics via web search...\")\n",
    "        recommendations_content = generate_recommendations(topic, recommendation_chain, model)\n",
    "        \n",
    "        full_report = create_full_report(topic, report_content, recommendations_content)\n",
    "        \n",
    "        report_filename = save_markdown_file(topic, full_report)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"report_content\": report_content,\n",
    "            \"recommendations_content\": recommendations_content,\n",
    "            \"full_report\": full_report,\n",
    "            \"report_filename\": report_filename,\n",
    "            \"success\": True\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Error researching topic: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def display_results(result: Dict[str, Any], display_type: str = \"report\"):\n",
    "    \"\"\"Display research results in a formatted way\"\"\"\n",
    "    if not result.get(\"success\", False):\n",
    "        print(f\"Error: {result.get('error', 'Unknown error occurred')}\")\n",
    "        return\n",
    "    \n",
    "    if display_type == \"report\":\n",
    "        print(f\"Research report saved to: {result.get('report_filename', 'Unknown')}\")\n",
    "        display_markdown(result.get(\"full_report\", \"\"))\n",
    "    \n",
    "    elif display_type == \"paper_analysis\":\n",
    "        print(f\"Paper analysis saved to: {result.get('report_filename', 'Unknown')}\")\n",
    "        display_markdown(result.get(\"full_analysis\", \"\"))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    print(\"AI Research Assistant\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    # Initialize environment\n",
    "    api_key = load_environment()\n",
    "    if not api_key:\n",
    "        print(\"Error: GEMINI_API_KEY not found in environment variables or .env file\")\n",
    "        return\n",
    "    \n",
    "    # Initialize model and components\n",
    "    model = initialize_model(api_key)\n",
    "    TopicRecommendations, PaperRecommendations = define_output_schemas()\n",
    "    report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt = create_prompt_templates()\n",
    "    report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain = create_chains(\n",
    "        model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt, \n",
    "        TopicRecommendations, PaperRecommendations\n",
    "    )\n",
    "    \n",
    "    # Initialize database\n",
    "    initialize_database()\n",
    "    \n",
    "    # Demo options\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"1. Research a topic\")\n",
    "    print(\"2. Analyze a research paper\")\n",
    "    print(\"3. Exit\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1-3): \")\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        topic = input(\"Enter research topic: \")\n",
    "        result = research_topic(topic, report_chain, recommendation_chain, model)\n",
    "        display_results(result, \"report\")\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        file_path = input(\"Enter path to PDF or DOCX file: \")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return\n",
    "            \n",
    "        result = process_research_paper(\n",
    "            file_path, \n",
    "            paper_summary_chain=paper_summary_chain,\n",
    "            paper_recommendation_chain=paper_recommendation_chain,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            full_analysis = create_full_paper_analysis(\n",
    "                result[\"filename\"],\n",
    "                result[\"summary\"],\n",
    "                result[\"recommendations\"]\n",
    "            )\n",
    "            \n",
    "            report_filename = save_markdown_file(\n",
    "                os.path.splitext(result[\"filename\"])[0],\n",
    "                full_analysis\n",
    "            )\n",
    "            \n",
    "            result[\"report_filename\"] = report_filename\n",
    "            result[\"full_analysis\"] = full_analysis\n",
    "            \n",
    "        display_results(result, \"paper_analysis\")\n",
    "    \n",
    "    elif choice == \"3\":\n",
    "        print(\"Exiting...\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Research Assistant\n",
      "--------------------\n",
      "\n",
      "Options:\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Ask questions about a paper (RAG)\n",
      "4. Exit\n",
      "\n",
      "You can now ask questions about the paper. Type 'exit' to quit.\n",
      "\n",
      "Answer:\n",
      "The section \"Why Deep Learning in Today's Research and Applications?\" addresses the importance of deep learning in the context of the Fourth Industrial Revolution (Industry 4.0). The paper states that the main focus of Industry 4.0 is \"typically technology-driven automation, smart and intelligent systems\" across various application areas like smart healthcare, business intelligence, smart cities, and cybersecurity intelligence.\n",
      "\n",
      "Deep learning (DL) is presented as a key technology for achieving the goals of Industry 4.0 because of its ability to uncover complex architectures in high-dimensional data and its excellent learning capabilities from historical data. Thus, DL techniques can play a key role in building intelligent data-driven systems. The paper also mentions that deep learning differs from standard machine learning in terms of efficiency as the volume of data increases.\n",
      "\n",
      "Options:\n",
      "1. Research a topic\n",
      "2. Analyze a research paper\n",
      "3. Ask questions about a paper (RAG)\n",
      "4. Exit\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import sqlite3\n",
    "import tempfile\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import PyPDF2 \n",
    "import docx  \n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_environment():\n",
    "    \"\"\"Load environment variables\"\"\"\n",
    "    load_dotenv()\n",
    "    return os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "def initialize_model(api_key):\n",
    "    \"\"\"Initialize Gemini model\"\"\"\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "    )\n",
    "\n",
    "def define_output_schemas():\n",
    "    \"\"\"Define output schemas for topic and paper recommendations\"\"\"\n",
    "    # Output schema for topic recommendations\n",
    "    class RecommendedTopic(BaseModel):\n",
    "        topic: str = Field(description=\"The name of the recommended topic\")\n",
    "        description: str = Field(description=\"A brief description of why this topic is relevant\")\n",
    "        resource_url: str = Field(description=\"A relevant resource URL for this topic\")\n",
    "\n",
    "    class TopicRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedTopic] = Field(description=\"List of recommended related topics\")\n",
    "\n",
    "    # Output schema for paper recommendations\n",
    "    class RecommendedPaper(BaseModel):\n",
    "        title: str = Field(description=\"The title of the recommended research paper\")\n",
    "        authors: str = Field(description=\"The authors of the paper\")\n",
    "        year: str = Field(description=\"Publication year\")\n",
    "        description: str = Field(description=\"Brief description of relevance to the original paper\")\n",
    "        paper_url: str = Field(description=\"URL to access this paper\", default=\"\")\n",
    "\n",
    "    class PaperRecommendations(BaseModel):\n",
    "        recommendations: List[RecommendedPaper] = Field(description=\"List of recommended related papers\")\n",
    "        \n",
    "    return TopicRecommendations, PaperRecommendations\n",
    "\n",
    "def create_prompt_templates():\n",
    "    \"\"\"Create prompt templates for research tasks\"\"\"\n",
    "    report_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a comprehensive, detailed report on the following topic:\n",
    "        \n",
    "        Topic: {topic}\n",
    "        \n",
    "        Your report should include:\n",
    "        1. Introduction to the topic\n",
    "        2. Key concepts and definitions\n",
    "        3. Historical context and development\n",
    "        4. Current state and applications\n",
    "        5. Future directions and potential developments\n",
    "        6. Conclusion\n",
    "        \n",
    "        Format your report with clear markdown headings and subheadings. Use proper markdown formatting for emphasis, lists, and other elements.\n",
    "        Make sure to provide in-depth analysis.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Based on the topic: {topic}\n",
    "        \n",
    "        Generate 5 relevant related topics that the user might be interested in researching next.\n",
    "        For each recommendation, provide:\n",
    "        1. The topic name\n",
    "        2. A brief 1-2 sentence description of why it's relevant\n",
    "        3. A relevant resource URL that would contain valuable information about this topic\n",
    "        \n",
    "        Your response must be formatted as a valid JSON object that matches this structure:\n",
    "        {\n",
    "            \"recommendations\": [\n",
    "                {\n",
    "                    \"topic\": \"Topic Name\",\n",
    "                    \"description\": \"Brief description of relevance\",\n",
    "                    \"resource_url\": \"https://example.com/relevant-page\"\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        Use reputable sources for your resource URLs. While you can't verify if the exact URLs exist,\n",
    "        make them realistic and likely to contain quality information.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an AI research assistant. Create a concise but comprehensive summary of the following research paper:\n",
    "        \n",
    "        Paper content: {paper_content}\n",
    "        \n",
    "        Your summary should include:\n",
    "        1. Main objective of the research\n",
    "        2. Methodology used\n",
    "        3. Key findings and results\n",
    "        4. Main conclusions and implications\n",
    "        5. Limitations (if mentioned)\n",
    "        \n",
    "        Format your summary with clear markdown headings and keep it concise yet informative.\n",
    "        Focus on the most important aspects of the paper.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    paper_recommendation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Based on the following research paper:\n",
    "    \n",
    "    Paper content: {paper_content}\n",
    "    \n",
    "    Generate 5 relevant related research papers that the user might be interested in reading next.\n",
    "    These should be real papers that likely exist in the academic literature.\n",
    "    \n",
    "    For each recommendation, provide:\n",
    "    1. The paper title (use the actual title of a real paper if you know it)\n",
    "    2. The authors (use \"et al.\" for multiple authors after the first)\n",
    "    3. Publication year (estimate if necessary)\n",
    "    4. A brief description of why it's relevant to the original paper\n",
    "    5. A URL where the paper might be found - THIS IS CRITICAL. \n",
    "    \n",
    "    For URLs, use specific links from:\n",
    "    - Google Scholar (https://scholar.google.com/scholar?q=PAPER_TITLE)\n",
    "    - arXiv (https://arxiv.org/search/?query=PAPER_TITLE)\n",
    "    - ResearchGate (https://www.researchgate.net/search.Search.html?query=PAPER_TITLE)\n",
    "    - ACM Digital Library (https://dl.acm.org/action/doSearch?AllField=PAPER_TITLE)\n",
    "    - IEEE Xplore (https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=PAPER_TITLE)\n",
    "    \n",
    "    Replace PAPER_TITLE with URL-encoded paper title in these templates. Make sure EVERY recommendation has a working URL.\n",
    "    \n",
    "    Your response must be formatted as a valid JSON object that matches this structure:\n",
    "    {{\n",
    "        \"recommendations\": [\n",
    "            {{\n",
    "                \"title\": \"Paper Title\",\n",
    "                \"authors\": \"Author names\",\n",
    "                \"year\": \"Publication year\",\n",
    "                \"description\": \"Brief description of relevance\",\n",
    "                \"paper_url\": \"https://example.com/paper-link\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "    \n",
    "    return report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt\n",
    "\n",
    "def create_chains(model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt, TopicRecommendations, PaperRecommendations):\n",
    "    \"\"\"Create processing chains for research tasks\"\"\"\n",
    "    report_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | report_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    recommendation_chain = (\n",
    "        {\"topic\": RunnablePassthrough()}\n",
    "        | recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=TopicRecommendations)\n",
    "    )\n",
    "\n",
    "    paper_summary_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_summary_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    paper_recommendation_chain = (\n",
    "        {\"paper_content\": RunnablePassthrough()}\n",
    "        | paper_recommendation_prompt\n",
    "        | model\n",
    "        | JsonOutputParser(pydantic_object=PaperRecommendations)\n",
    "    )\n",
    "    \n",
    "    return report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain\n",
    "\n",
    "def initialize_database(db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Initialize SQLite database for storing papers\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS papers (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            filename TEXT NOT NULL,\n",
    "            content TEXT NOT NULL,\n",
    "            file_type TEXT NOT NULL,\n",
    "            upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            summary TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_file_to_database(filename: str, content: str, file_type: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save file content to SQLite database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO papers (filename, content, file_type) VALUES (?, ?, ?)\",\n",
    "        (filename, content, file_type)\n",
    "    )\n",
    "    paper_id = cursor.lastrowid\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return paper_id\n",
    "\n",
    "def save_summary_to_database(paper_id: int, summary: str, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Save paper summary to database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"UPDATE papers SET summary = ? WHERE id = ?\",\n",
    "        (summary, paper_id)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_paper_from_database(paper_id: int, db_path: str = \"../data/research_papers.db\"):\n",
    "    \"\"\"Retrieve paper content from database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT filename, content, file_type, summary FROM papers WHERE id = ?\", (paper_id,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    if result:\n",
    "        return {\n",
    "            \"filename\": result[0],\n",
    "            \"content\": result[1],\n",
    "            \"file_type\": result[2],\n",
    "            \"summary\": result[3]\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "        # Fallback method\n",
    "        text = \"\"\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text content from a DOCX file\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from DOCX: {str(e)}\")\n",
    "        # Fallback method\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\"\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "def generate_report(topic: str, report_chain) -> str:\n",
    "    \"\"\"Generate a detailed report on the given topic\"\"\"\n",
    "    return report_chain.invoke(topic)\n",
    "\n",
    "def search_web(query, num_results=5):\n",
    "    \"\"\"Search the web for related topics\"\"\"\n",
    "    try:\n",
    "        # Format query for search engines\n",
    "        search_query = urllib.parse.quote_plus(query)\n",
    "        \n",
    "        # List of search URLs to try\n",
    "        search_urls = [\n",
    "            f\"https://www.google.com/search?q={search_query}\",\n",
    "            f\"https://en.wikipedia.org/wiki/Special:Search?search={search_query}&go=Go\",\n",
    "            f\"https://scholar.google.com/scholar?q={search_query}\"\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Set user agent to avoid being blocked\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Try each search engine until we get enough results\n",
    "        for search_url in search_urls:\n",
    "            if len(results) >= num_results:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                response = requests.get(search_url, headers=headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    # Extract links and titles (implementation varies by search engine)\n",
    "                    if 'google.com/search' in search_url:\n",
    "                        # For Google\n",
    "                        search_results = soup.select('div.g')\n",
    "                        for result in search_results:\n",
    "                            title_element = result.select_one('h3')\n",
    "                            link_element = result.select_one('a')\n",
    "                            \n",
    "                            if title_element and link_element and 'href' in link_element.attrs:\n",
    "                                title = title_element.get_text()\n",
    "                                link = link_element['href']\n",
    "                                \n",
    "                                # Remove Google redirects\n",
    "                                if link.startswith('/url?q='):\n",
    "                                    link = link.split('/url?q=')[1].split('&')[0]\n",
    "                                \n",
    "                                if link.startswith('http') and not any(x['url'] == link for x in results):\n",
    "                                    results.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link\n",
    "                                    })\n",
    "                                    \n",
    "                                if len(results) >= num_results:\n",
    "                                    break\n",
    "                    \n",
    "                    elif 'wikipedia.org' in search_url:\n",
    "                        # For Wikipedia\n",
    "                        search_results = soup.select('ul.mw-search-results li')\n",
    "                        for result in search_results:\n",
    "                            title_element = result.select_one('a')\n",
    "                            if title_element and 'href' in title_element.attrs:\n",
    "                                title = title_element.get_text()\n",
    "                                link = 'https://en.wikipedia.org' + title_element['href']\n",
    "                                \n",
    "                                if not any(x['url'] == link for x in results):\n",
    "                                    results.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link\n",
    "                                    })\n",
    "                                    \n",
    "                                if len(results) >= num_results:\n",
    "                                    break\n",
    "                    \n",
    "                    elif 'scholar.google.com' in search_url:\n",
    "                        # For Google Scholar\n",
    "                        search_results = soup.select('div.gs_ri')\n",
    "                        for result in search_results:\n",
    "                            title_element = result.select_one('h3 a')\n",
    "                            if title_element and 'href' in title_element.attrs:\n",
    "                                title = title_element.get_text()\n",
    "                                link = title_element['href']\n",
    "                                \n",
    "                                if not link.startswith('http'):\n",
    "                                    link = 'https://scholar.google.com' + link\n",
    "                                \n",
    "                                if not any(x['url'] == link for x in results):\n",
    "                                    results.append({\n",
    "                                        'title': title,\n",
    "                                        'url': link\n",
    "                                    })\n",
    "                                    \n",
    "                                if len(results) >= num_results:\n",
    "                                    break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error searching {search_url}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Web search error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def generate_recommendations(topic: str, recommendation_chain=None, model=None) -> str:\n",
    "    \"\"\"Generate relevant topic recommendations using web crawling\"\"\"\n",
    "    try:\n",
    "        print(\"- Searching the web for related topics...\")\n",
    "        search_results = search_web(topic, num_results=15)\n",
    "        \n",
    "        # If we didn't get enough results, try searching for \"related to [topic]\"\n",
    "        if len(search_results) < 5:\n",
    "            additional_results = search_web(f\"related to {topic}\", num_results=10)\n",
    "            for result in additional_results:\n",
    "                if result not in search_results:\n",
    "                    search_results.append(result)\n",
    "        \n",
    "        # Extract relevant topics from search results\n",
    "        formatted_recommendations = \"# Related Topics You May Be Interested In\\n\\n\"\n",
    "        used_topics = set()\n",
    "        count = 0\n",
    "        \n",
    "        for result in search_results:\n",
    "            if count >= 5:\n",
    "                break\n",
    "                \n",
    "            # Extract a topic name from the search result title\n",
    "            title = result['title']\n",
    "            url = result['url']\n",
    "            \n",
    "            # Skip if URL is suspicious\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                continue\n",
    "                \n",
    "            # Generate a clean topic name\n",
    "            topic_name = re.sub(r'\\s*\\|.*$', '', title)  # Remove text after pipe symbol\n",
    "            topic_name = re.sub(r'\\s*-.*$', '', topic_name)  # Remove text after dash\n",
    "            \n",
    "            # Skip very short or very long topic names\n",
    "            if len(topic_name) < 5 or len(topic_name) > 100:\n",
    "                continue\n",
    "                \n",
    "            # Skip if too similar to original topic\n",
    "            if topic_name.lower() == topic.lower():\n",
    "                continue\n",
    "                \n",
    "            # Check if we've already used a similar topic\n",
    "            similar = False\n",
    "            for used_topic in used_topics:\n",
    "                if (topic_name.lower() in used_topic.lower() or \n",
    "                    used_topic.lower() in topic_name.lower()):\n",
    "                    similar = True\n",
    "                    break\n",
    "            \n",
    "            if similar:\n",
    "                continue\n",
    "                \n",
    "            used_topics.add(topic_name)\n",
    "            count += 1\n",
    "            \n",
    "            # Generate a description using the topic and original query\n",
    "            description = f\"This topic is closely related to {topic} and offers additional perspectives and insights.\"\n",
    "            \n",
    "            formatted_recommendations += f\"## {count}. {topic_name}\\n\"\n",
    "            formatted_recommendations += f\"{description}\\n\"\n",
    "            formatted_recommendations += f\"[Learn more]({url})\\n\\n\"\n",
    "        \n",
    "        # If we didn't get enough recommendations from web search, generate the missing ones\n",
    "        if count < 5:\n",
    "            # Create a backup prompt for generating the remaining recommendations\n",
    "            backup_prompt = ChatPromptTemplate.from_template(\n",
    "                f\"\"\"\n",
    "                I already have the following related topics for \"{topic}\":\n",
    "                {\", \".join(used_topics)}\n",
    "                \n",
    "                Please suggest {5 - count} more related topics that are different from the ones above.\n",
    "                For each topic, provide:\n",
    "                1. The topic name\n",
    "                2. A brief description of why it's relevant to {topic}\n",
    "                3. A reasonable URL where someone might learn about this topic (like a Wikipedia or educational site)\n",
    "                \n",
    "                Format each recommendation like this:\n",
    "                Topic: [topic name]\n",
    "                Description: [description]\n",
    "                URL: [url]\n",
    "                \"\"\"\n",
    "            )\n",
    "            backup_chain = backup_prompt | model | StrOutputParser()\n",
    "            additional_recs = backup_chain.invoke({})\n",
    "            \n",
    "            # Parse the generated recommendations\n",
    "            for line in additional_recs.split(\"\\n\\n\"):\n",
    "                if count >= 5:\n",
    "                    break\n",
    "                    \n",
    "                match_topic = re.search(r\"Topic:(.+)\", line)\n",
    "                match_desc = re.search(r\"Description:(.+)\", line)\n",
    "                match_url = re.search(r\"URL:(.+)\", line)\n",
    "                \n",
    "                if match_topic and match_desc and match_url:\n",
    "                    topic_name = match_topic.group(1).strip()\n",
    "                    description = match_desc.group(1).strip()\n",
    "                    url = match_url.group(1).strip()\n",
    "                    \n",
    "                    count += 1\n",
    "                    formatted_recommendations += f\"## {count}. {topic_name}\\n\"\n",
    "                    formatted_recommendations += f\"{description}\\n\"\n",
    "                    formatted_recommendations += f\"[Learn more]({url})\\n\\n\"\n",
    "        \n",
    "        return formatted_recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in web-based recommendations: {str(e)}\")\n",
    "        # Fallback to using the LLM directly if web search fails\n",
    "        backup_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Based on the topic: {topic}\n",
    "            \n",
    "            Provide 5 relevant related topics that the user might be interested in researching next.\n",
    "            For each recommendation, provide:\n",
    "            1. The topic name\n",
    "            2. A brief description of why it's relevant\n",
    "            3. A relevant resource link\n",
    "            \n",
    "            Format your response as a markdown list.\n",
    "            \"\"\"\n",
    "        )\n",
    "        backup_chain = backup_prompt | model | StrOutputParser()\n",
    "        return backup_chain.invoke({\"topic\": topic})\n",
    "\n",
    "def generate_paper_recommendations(paper_content, paper_recommendation_chain=None, model=None) -> str:\n",
    "    \"\"\"Generate recommendations for related papers using web crawling\"\"\"\n",
    "    try:\n",
    "        # Extract key phrases from the paper\n",
    "        key_phrases_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Extract 5 key technical phrases or terms from the following paper that could be used to find related research.\n",
    "            Only return the phrases as a comma-separated list with no additional text.\n",
    "            \n",
    "            Paper content: {paper_content}\n",
    "            \"\"\"\n",
    "        )\n",
    "        key_phrases_chain = key_phrases_prompt | model | StrOutputParser()\n",
    "        key_phrases = key_phrases_chain.invoke({\"paper_content\": paper_content}).split(\",\")\n",
    "        \n",
    "        # Search for related papers using the key phrases\n",
    "        papers = []\n",
    "        for phrase in key_phrases:\n",
    "            if len(papers) >= 10:\n",
    "                break\n",
    "                \n",
    "            search_results = search_web(f\"{phrase.strip()} research paper\", num_results=5)\n",
    "            for result in search_results:\n",
    "                if len(papers) >= 10:\n",
    "                    break\n",
    "                \n",
    "                title = result['title']\n",
    "                url = result['url']\n",
    "                \n",
    "                # Skip non-academic-looking results\n",
    "                if not any(domain in url for domain in ['.edu', 'arxiv.org', 'scholar.google', 'researchgate', 'ieee.org', 'acm.org']):\n",
    "                    continue\n",
    "                    \n",
    "                # Skip very short titles\n",
    "                if len(title) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if we already have this paper\n",
    "                if any(p['title'] == title for p in papers):\n",
    "                    continue\n",
    "                    \n",
    "                # Add the paper to our results\n",
    "                papers.append({\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'phrase': phrase.strip()\n",
    "                })\n",
    "        \n",
    "        # Process the best 5 results\n",
    "        top_papers = papers[:5]\n",
    "        \n",
    "        # If we don't have enough papers, generate some with the model\n",
    "        if len(top_papers) < 5:\n",
    "            remaining = 5 - len(top_papers)\n",
    "            paper_gen_prompt = ChatPromptTemplate.from_template(\n",
    "                f\"\"\"\n",
    "                Based on the key phrases {', '.join(key_phrases)}, \n",
    "                suggest {remaining} academic papers that would be related to a paper discussing these topics.\n",
    "                \n",
    "                For each paper, provide:\n",
    "                1. A realistic paper title\n",
    "                2. Author names (use et al. for multiple authors)\n",
    "                3. A realistic publication year (between 2015-2024)\n",
    "                4. A URL where the paper might be found\n",
    "                \n",
    "                Format each paper like this:\n",
    "                Title: [title]\n",
    "                Authors: [authors]\n",
    "                Year: [year]\n",
    "                URL: [url]\n",
    "                \"\"\"\n",
    "            )\n",
    "            paper_gen_chain = paper_gen_prompt | model | StrOutputParser()\n",
    "            additional_papers = paper_gen_chain.invoke({})\n",
    "            \n",
    "            # Parse the generated papers\n",
    "            current_paper = {}\n",
    "            for line in additional_papers.split('\\n'):\n",
    "                if line.startswith('Title:'):\n",
    "                    if current_paper and 'title' in current_paper:\n",
    "                        top_papers.append(current_paper)\n",
    "                        current_paper = {}\n",
    "                    current_paper['title'] = line.replace('Title:', '').strip()\n",
    "                elif line.startswith('Authors:'):\n",
    "                    current_paper['authors'] = line.replace('Authors:', '').strip()\n",
    "                elif line.startswith('Year:'):\n",
    "                    current_paper['year'] = line.replace('Year:', '').strip()\n",
    "                elif line.startswith('URL:'):\n",
    "                    current_paper['url'] = line.replace('URL:', '').strip()\n",
    "            \n",
    "            if current_paper and 'title' in current_paper:\n",
    "                top_papers.append(current_paper)\n",
    "        \n",
    "        # Generate descriptions for each paper\n",
    "        formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "        for i, paper in enumerate(top_papers[:5], 1):\n",
    "            title = paper.get('title', '')\n",
    "            url = paper.get('url', '')\n",
    "            \n",
    "            # Generate paper metadata if missing\n",
    "            authors = paper.get('authors', 'Various authors')\n",
    "            if 'authors' not in paper:\n",
    "                # Extract authors from title or URL if possible, otherwise use placeholder\n",
    "                authors = \"Various authors\"\n",
    "                \n",
    "            year = paper.get('year', '2023')\n",
    "            if 'year' not in paper:\n",
    "                # Try to extract year from URL or title, otherwise use recent year\n",
    "                year_match = re.search(r'20[12]\\d', title)\n",
    "                if year_match:\n",
    "                    year = year_match.group(0)\n",
    "                else:\n",
    "                    year = \"2023\"\n",
    "            \n",
    "            # Generate a description based on the title and the original key phrase\n",
    "            description = f\"This paper relates to {paper.get('phrase', 'your research topic')} and expands on the concepts in your paper.\"\n",
    "            \n",
    "            # Add the recommendation\n",
    "            formatted_recommendations += f\"## {i}. {title} ({year})\\n\"\n",
    "            formatted_recommendations += f\"**Authors:** {authors}\\n\\n\"\n",
    "            formatted_recommendations += f\"{description}\\n\"\n",
    "            formatted_recommendations += f\"[Access Paper]({url})\\n\\n\"\n",
    "            \n",
    "        return formatted_recommendations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in web-based paper recommendations: {str(e)}\")\n",
    "        # Fallback to using the paper recommendation chain\n",
    "        try:\n",
    "            recommendations_data = paper_recommendation_chain.invoke(paper_content)\n",
    "            recs = recommendations_data[\"recommendations\"]  # Access as a dict\n",
    "            formatted_recommendations = \"# Related Research Papers You May Be Interested In\\n\\n\"\n",
    "            for i, rec in enumerate(recs, 1):\n",
    "                formatted_recommendations += f\"## {i}. {rec['title']} ({rec['year']})\\n\"\n",
    "                formatted_recommendations += f\"**Authors:** {rec['authors']}\\n\\n\"\n",
    "                formatted_recommendations += f\"{rec['description']}\\n\"\n",
    "                paper_url = rec['paper_url'].strip()\n",
    "                if not paper_url:\n",
    "                    encoded_title = re.sub(r'\\s+', '+', rec['title'])\n",
    "                    paper_url = f\"https://scholar.google.com/scholar?q={encoded_title}\"\n",
    "                formatted_recommendations += f\"[Access Paper]({paper_url})\\n\\n\"\n",
    "            return formatted_recommendations\n",
    "        except Exception as e:\n",
    "            # If everything fails, use a simple backup prompt\n",
    "            backup_prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                Based on the following research paper content:\n",
    "                \n",
    "                {paper_content}\n",
    "                \n",
    "                Provide 5 relevant related research papers that might be of interest.\n",
    "                For each paper, include:\n",
    "                1. Title (a real paper title if possible)\n",
    "                2. Authors\n",
    "                3. Year\n",
    "                4. Brief description of relevance\n",
    "                5. A direct URL to access the paper\n",
    "                \n",
    "                Format your response in markdown with clear headings and clickable links.\n",
    "                \"\"\"\n",
    "            )\n",
    "            backup_chain = backup_prompt | model | StrOutputParser()\n",
    "            return backup_chain.invoke({\"paper_content\": paper_content})\n",
    "\n",
    "def create_full_report(topic: str, report_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report combining the report and recommendations\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Report: {topic}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "{report_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This report was generated by AI Research Assistant*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def create_full_paper_analysis(filename: str, summary_content: str, recommendations_content: str) -> str:\n",
    "    \"\"\"Create a full markdown report for paper analysis\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_report = f\"\"\"\n",
    "# Research Paper Analysis: {filename}\n",
    "\n",
    "*Generated on: {timestamp}*\n",
    "\n",
    "---\n",
    "\n",
    "## Paper Summary\n",
    "\n",
    "{summary_content}\n",
    "\n",
    "---\n",
    "\n",
    "{recommendations_content}\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis was generated by AI Research Assistant*\n",
    "\"\"\"\n",
    "    return full_report\n",
    "\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Convert a string to a valid filename\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "def save_markdown_file(topic: str, content: str) -> str:\n",
    "    \"\"\"Save content to a markdown file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    safe_topic = sanitize_filename(topic)\n",
    "    filename = f\"research_{safe_topic}_{timestamp}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return filename\n",
    "\n",
    "def display_markdown(content: str, use_markdown_display: bool = True):\n",
    "    \"\"\"Display content as rendered markdown if in IPython environment\"\"\"\n",
    "    try:\n",
    "        if use_markdown_display:\n",
    "            display(Markdown(content))\n",
    "        else:\n",
    "            print(content)\n",
    "    except:\n",
    "        print(content)\n",
    "\n",
    "def process_research_paper(file_path: str, original_filename: Optional[str] = None, \n",
    "                           db_path: str = \"../data/research_papers.db\", \n",
    "                           paper_summary_chain=None, \n",
    "                           paper_recommendation_chain=None, \n",
    "                           model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Process a research paper file (PDF or DOCX)\"\"\"\n",
    "    if not original_filename:\n",
    "        original_filename = os.path.basename(file_path)\n",
    "    file_extension = os.path.splitext(original_filename)[1].lower()\n",
    "    if file_extension == '.pdf':\n",
    "        text_content = extract_text_from_pdf(file_path)\n",
    "        file_type = 'pdf'\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        text_content = extract_text_from_docx(file_path)\n",
    "        file_type = 'docx'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=12000,\n",
    "        chunk_overlap=2000\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_content)\n",
    "    processing_text = chunks[0] if len(chunks) > 0 else text_content\n",
    "    \n",
    "    paper_id = save_file_to_database(original_filename, text_content, file_type, db_path=db_path)\n",
    "    \n",
    "    try:\n",
    "        print(\"- Generating research paper summary...\")\n",
    "        summary = paper_summary_chain.invoke(processing_text)\n",
    "        save_summary_to_database(paper_id, summary, db_path=db_path)\n",
    "        \n",
    "        print(\"- Finding related research papers via web search...\")\n",
    "        formatted_recommendations = generate_paper_recommendations(processing_text, paper_recommendation_chain, model)\n",
    "        \n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"summary\": summary,\n",
    "            \"recommendations\": formatted_recommendations,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing research paper: {str(e)}\")\n",
    "\n",
    "        return {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"filename\": original_filename,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def upload_research_paper_file(file_data, filename: str, \n",
    "                               paper_summary_chain=None, \n",
    "                               paper_recommendation_chain=None, \n",
    "                               model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Process an uploaded research paper file\"\"\"\n",
    "    try:\n",
    "        # Create a temporary file to work with\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:\n",
    "            temp_file.write(file_data)\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        # Process the paper\n",
    "        result = process_research_paper(\n",
    "            temp_path, \n",
    "            original_filename=filename,\n",
    "            paper_summary_chain=paper_summary_chain,\n",
    "            paper_recommendation_chain=paper_recommendation_chain,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_path)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            # Create full analysis report\n",
    "            full_analysis = create_full_paper_analysis(\n",
    "                filename,\n",
    "                result[\"summary\"],\n",
    "                result[\"recommendations\"]\n",
    "            )\n",
    "            \n",
    "            # Save the full analysis to a file\n",
    "            report_filename = save_markdown_file(\n",
    "                os.path.splitext(filename)[0],\n",
    "                full_analysis\n",
    "            )\n",
    "            \n",
    "            result[\"report_filename\"] = report_filename\n",
    "            result[\"full_analysis\"] = full_analysis\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Error processing uploaded file: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def research_topic(topic: str, report_chain=None, recommendation_chain=None, model=None) -> Dict[str, Any]:\n",
    "    \"\"\"Conduct research on a given topic\"\"\"\n",
    "    try:\n",
    "        print(f\"Researching topic: {topic}\")\n",
    "        \n",
    "        print(\"- Generating detailed report...\")\n",
    "        report_content = generate_report(topic, report_chain)\n",
    "        \n",
    "        print(\"- Finding related topics via web search...\")\n",
    "        recommendations_content = generate_recommendations(topic, recommendation_chain, model)\n",
    "        \n",
    "        full_report = create_full_report(topic, report_content, recommendations_content)\n",
    "        \n",
    "        report_filename = save_markdown_file(topic, full_report)\n",
    "        \n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"report_content\": report_content,\n",
    "            \"recommendations_content\": recommendations_content,\n",
    "            \"full_report\": full_report,\n",
    "            \"report_filename\": report_filename,\n",
    "            \"success\": True\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Error researching topic: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def display_results(result: Dict[str, Any], display_type: str = \"report\"):\n",
    "    \"\"\"Display research results in a formatted way\"\"\"\n",
    "    if not result.get(\"success\", False):\n",
    "        print(f\"Error: {result.get('error', 'Unknown error occurred')}\")\n",
    "        return\n",
    "    \n",
    "    if display_type == \"report\":\n",
    "        print(f\"Research report saved to: {result.get('report_filename', 'Unknown')}\")\n",
    "        display_markdown(result.get(\"full_report\", \"\"))\n",
    "    \n",
    "    elif display_type == \"paper_analysis\":\n",
    "        print(f\"Paper analysis saved to: {result.get('report_filename', 'Unknown')}\")\n",
    "        display_markdown(result.get(\"full_analysis\", \"\"))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    print(\"AI Research Assistant\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    # Initialize environment\n",
    "    api_key = load_environment()\n",
    "    if not api_key:\n",
    "        print(\"Error: GEMINI_API_KEY not found in environment variables or .env file\")\n",
    "        return\n",
    "    \n",
    "    # Initialize model and components\n",
    "    model = initialize_model(api_key)\n",
    "    TopicRecommendations, PaperRecommendations = define_output_schemas()\n",
    "    report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt = create_prompt_templates()\n",
    "    report_chain, recommendation_chain, paper_summary_chain, paper_recommendation_chain = create_chains(\n",
    "        model, report_prompt, recommendation_prompt, paper_summary_prompt, paper_recommendation_prompt, \n",
    "        TopicRecommendations, PaperRecommendations\n",
    "    )\n",
    "    \n",
    "    # Initialize database\n",
    "    initialize_database()\n",
    "    \n",
    "    # Demo options\n",
    "    while True:\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1. Research a topic\")\n",
    "        print(\"2. Analyze a research paper\")\n",
    "        print(\"3. Ask questions about a paper (RAG)\")\n",
    "        print(\"4. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter choice (1-4): \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            topic = input(\"Enter research topic: \")\n",
    "            result = research_topic(topic, report_chain, recommendation_chain, model)\n",
    "            display_results(result, \"report\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            file_path = input(\"Enter path to PDF or DOCX file: \")\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            result = process_research_paper(\n",
    "                file_path, \n",
    "                paper_summary_chain=paper_summary_chain,\n",
    "                paper_recommendation_chain=paper_recommendation_chain,\n",
    "                model=model\n",
    "            )\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                full_analysis = create_full_paper_analysis(\n",
    "                    result[\"filename\"],\n",
    "                    result[\"summary\"],\n",
    "                    result[\"recommendations\"]\n",
    "                )\n",
    "                \n",
    "                report_filename = save_markdown_file(\n",
    "                    os.path.splitext(result[\"filename\"])[0],\n",
    "                    full_analysis\n",
    "                )\n",
    "                \n",
    "                result[\"report_filename\"] = report_filename\n",
    "                result[\"full_analysis\"] = full_analysis\n",
    "                \n",
    "            display_results(result, \"paper_analysis\")\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            file_path = input(\"Enter path to research paper (PDF/DOCX): \").strip()\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract text\n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            text_content = \"\"\n",
    "            try:\n",
    "                if file_extension == '.pdf':\n",
    "                    text_content = extract_text_from_pdf(file_path)\n",
    "                elif file_extension in ['.docx', '.doc']:\n",
    "                    text_content = extract_text_from_docx(file_path)\n",
    "                else:\n",
    "                    print(\"Unsupported file format. Please upload a PDF or DOCX file.\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Split text into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text_content)\n",
    "            \n",
    "            # Create embeddings and vector store\n",
    "            try:\n",
    "                embeddings = GoogleGenerativeAIEmbeddings(\n",
    "                    model=\"models/embedding-001\",\n",
    "                    google_api_key=api_key\n",
    "                )\n",
    "                vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "                retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating search index: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Create RAG chain\n",
    "            rag_prompt = ChatPromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "                You are an AI research assistant specializing in academic papers. Your task is to provide detailed and accurate answers to questions about research papers.\n",
    "                \n",
    "                Relevant sections from the paper:\n",
    "                {context}\n",
    "                \n",
    "                Question: {question}\n",
    "                \n",
    "                Instructions:\n",
    "                1. Provide a comprehensive answer based on the content from the paper.\n",
    "                2. Include specific details, explanations, and examples from the paper when relevant.\n",
    "                3. If appropriate, mention figures, tables, or specific sections referenced in the text.\n",
    "                4. If the question cannot be answered from the provided content, explain why and what information might be needed.\n",
    "                5. Use a clear, academic style appropriate for discussing research.\n",
    "                6. Structure your answer with paragraphs for readability.\n",
    "                7. If you quote directly from the paper, indicate this with quotation marks.\n",
    "                \n",
    "                Your detailed answer:\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            rag_chain = (\n",
    "                {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "                | rag_prompt\n",
    "                | model\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "            \n",
    "            print(\"\\nYou can now ask questions about the paper. Type 'exit' to quit.\")\n",
    "            while True:\n",
    "                question = input(\"\\nYour question: \").strip()\n",
    "                if question.lower() in ['exit', 'quit']:\n",
    "                    break\n",
    "                if not question:\n",
    "                    continue\n",
    "                try:\n",
    "                    answer = rag_chain.invoke(question)\n",
    "                    print(\"\\nAnswer:\")\n",
    "                    print(answer)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating answer: {e}\")\n",
    "        \n",
    "        elif choice == \"4\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
